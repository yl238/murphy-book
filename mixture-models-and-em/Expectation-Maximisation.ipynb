{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An alterative view of EM\n",
    "\n",
    "The goal of the EM algorithm is to find maximum likelihood solutions for models having latent variables. We denote the set of all observed data by $\\mathbf{X}$, in which the $n$-th row represents $\\mathbf{x}_n^T$, and similarly we denote the set of all latent variables by $\\mathbf{Z}$, with a corresponding row $\\mathbf{z}_n^T$. The set of all model parameters is denoted by $\\boldsymbol{\\theta}$, and so the log likelihood function is given by\n",
    "\n",
    "$$\n",
    "\\log p(\\mathbf{X}|\\boldsymbol{\\theta}) = \\log\\left\\{\\sum_{\\mathbf{Z}}p(\\mathbf{X}, \\mathbf{Z}|\\boldsymbol{\\theta})\\right\\}\n",
    "$$\n",
    "\n",
    "This also applies to continuous latent variables simply by replacing the sum over $\\mathbf{Z}$ with an integral.\n",
    "\n",
    "Note the summation over the latent variable appears inside the logarithm. Even if the joint distribution $p(\\mathbf{X}, \\mathbf{Z}|\\boldsymbol{\\theta})$ belongs to the exponential family, the marginal distribution $p(\\mathbf{X}|\\boldsymbol{\\theta})$ typically does not as a result of this summation. The presence of the sum prevents the logarithm from acting directly on the joint distribution, resulting in complicated expressions for the maximum likelihood solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose that, for each observation in $\\mathbf{X}$, we were told the corresponding value of the latent variable $\\mathbf{Z}$. We shall call $\\{\\mathbf{X}, \\mathbf{Z}\\}$ the *complete* data set, and we shall refer to the actual observed data $\\mathbf{X}$ as *incomplete*. The likelihood function for the complete dataset takes the form $\\log p(\\mathbf{X}, \\mathbf{Z}|\\boldsymbol{\\theta})$, and we shall suppose that maximization of this complete-data log likelihood function is straightforward.\n",
    "\n",
    "In practice, we are not given the complete data set $\\{\\mathbf{X}, \\mathbf{Z}\\}$, but only the incomplete data $\\mathbf{X}$. Our state of knowledge of the values of the latent variables in $\\mathbf{Z}$ is given only by the posterior distribution $p(\\mathbf{Z}|\\mathbf{X},\\boldsymbol{\\theta})$. Because we cannot use the complete-data log likelihood, we consider instead its expected value under the posterior distribution of the latent variable, which corresponds to the E step of the EM algorithm. In the subsequent M step, we maximize this expectation. If the current estimate for the parameters is denoted $\\boldsymbol{\\theta}^{\\mathrm{old}}$, then a pair of successive E and M steps gives rise to a revised estimate $\\boldsymbol{\\theta}^{\\mathrm{new}}$. The algorithm is initialized by choosing some starting value for the parameters $\\boldsymbol{\\theta}_0$. \n",
    "\n",
    "In the E step, we use the current parameter values $\\boldsymbol{\\theta}^{\\mathrm{old}}$ to find the posterior distribution of the latent variables given by $p(\\mathbf{Z}|\\mathbf{X},\\boldsymbol{\\theta}^{\\mathrm{old}})$. We then use this posterior distribution to find the expectation of the complete-data log likelihood evaluated for some general parameter value $\\boldsymbol{\\theta}$. This expectation, denoted $\\mathcal{Q}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\mathrm{old}})$, is given by \n",
    "\n",
    "$$\n",
    "\\mathcal{Q}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\mathrm{old}}) = \\sum_{\\mathbf{Z}}p(\\mathbf{Z}|\\mathbf{X},\\boldsymbol{\\theta}^{\\mathrm{old}})\\log p(\\mathbf{X}, \\mathbf{Z}|\\boldsymbol{\\theta}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the M step, we determine the revised parameter estimate $\\boldsymbol{\\theta}^{\\mathrm{new}}$ by maximising this function\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\theta}^{\\mathrm{new}} = \\mathrm{argmax}_{\\theta}\\mathcal{Q}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\mathrm{old}}).\n",
    "$$\n",
    "\n",
    "Note that in the definition of $\\mathcal{Q}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\mathrm{old}})$, the logarithm acts directly on the joint distribution $p(\\mathbf{X},\\mathbf{Z}|\\boldsymbol{\\theta})$, and so the corresponding M-step maximisation will by supposition be tractable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The general EM Algorithm\n",
    "> Given a joint distribution $p(\\mathbf{X},\\mathbf{Z}|\\boldsymbol{\\theta})$ over observed variables $\\mathbf{X}$ and latent variables $\\mathbf{Z}$, governed by the parameters $\\boldsymbol{\\theta}$, the goal is to maximise the likelihood function $p(\\mathbf{X}|\\boldsymbol{\\theta})$ with respect to $\\boldsymbol{\\theta}$.\n",
    "1. Choose an initial setting for the parameters $\\boldsymbol{\\theta}^{\\mathrm{old}}$.\n",
    "2. **E step** Evaluate $p(\\mathbf{Z}|\\mathbf{X}, \\boldsymbol{\\theta}^{\\mathrm{old}})$.\n",
    "3. **M step** Evaluate $\\boldsymbol{\\theta}^{\\mathrm{new}}$ given by \n",
    "$$\n",
    "\\boldsymbol{\\theta}^{\\mathrm{new}} = \\mathrm{argmax}_{\\theta}\\mathcal{Q}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\mathrm{old}}).\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\mathcal{Q}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\mathrm{old}}) = \\sum_{\\mathbf{Z}}p(\\mathbf{Z}|\\mathbf{X},\\boldsymbol{\\theta}^{\\mathrm{old}})\\log p(\\mathbf{X}, \\mathbf{Z}|\\boldsymbol{\\theta}).\n",
    "$$\n",
    "4. Check for convergence of either the log likelihood or the parameter values. If convergence criterion is not satisfied, then let\n",
    "$$\\boldsymbol{\\theta}^{\\mathrm{old}}\\leftarrow\\boldsymbol{\\theta}^{\\mathrm{new}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more general discussion\n",
    "Recall that our goal is to maximise the likelihood function that is given by\n",
    "\n",
    "$$\n",
    "p(\\mathbf{X}|\\boldsymbol{\\theta}) = \\sum_{\\mathbf{Z}}p(\\mathbf{X}, \\mathbf{Z}|\\boldsymbol{\\theta})\n",
    "$$\n",
    "\n",
    "Here we assume $\\mathbf{Z}$ is discrete. We shall suppose that direct optimization of $p(\\mathbf{X}|\\boldsymbol{\\theta})$ is difficult, but that optimization of the complete-data likelihood function $p(\\mathbf{X},\\mathbf{Z}|\\boldsymbol{\\theta})$ is significantly easier. \n",
    "\n",
    "Next we introduce a distribution $q(\\mathbf{Z})$ defined over the latent variables, and we observe that for any choice of $q(\\mathbf{Z})$, the following decomposition holds:\n",
    "\n",
    "$$\n",
    "\\log p(\\mathbf{X}|\\boldsymbol{\\theta}) = \\mathcal{L}(q, \\boldsymbol{\\theta}) + \\mathrm{KL}(q||p)\n",
    "$$\n",
    "\n",
    "where we have defined\n",
    "\\begin{eqnarray}\n",
    "\\mathcal{L}(q, \\boldsymbol{\\theta}) &=&\\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\log\\left\\{\\frac{p(\\mathbf{X},\\mathbf{Z}|\\boldsymbol{\\theta})}{q(\\mathbf{Z})}\\right\\}\\\\\n",
    "\\mathrm{KL}(q||p) & = & -\\sum_{\\mathbf{Z}}q(\\mathbf{Z})\\log\\left\\{\\frac{p(\\mathbf{Z}|\\mathbf{X}, \\boldsymbol{\\theta})}{q(\\mathbf{Z})}\\right\\}.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that they differ in sign and also that $\\mathcal{L}(q,\\boldsymbol{\\theta})$ contains the joint distribution of $\\mathbf{X}$ and $\\mathbf{Z}$ while $\\mathrm{KL}(q||p)$ contains the conditional distribution of $\\mathbf{Z}$ given $\\mathbf{X}$. To show this we need to use the product rule of probability:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{X},\\mathbf{Z}|\\boldsymbol{\\theta}) = p(\\mathbf{Z}|\\mathbf{X},\\boldsymbol{\\theta})p(\\mathbf{X}|\\boldsymbol{\\theta})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathrm{KL}(q||p)$ is the Kullback-Leibler divergence between $q(\\mathbf{Z})$ and the posterior distribution $p(\\mathbf{Z}|\\mathbf{X},\\boldsymbol{\\theta})$. Recall that $\\mathrm{KL}(q||p) \\ge 0$, with equality iff $q(\\mathbf{Z}) = p(\\mathbf{Z}|\\mathbf{X}, \\boldsymbol{\\theta})$. It follows that $\\mathcal{L}(q, \\boldsymbol{\\theta})\\le \\log p(\\mathbf{X}|\\boldsymbol{\\theta})$, i.e. $\\mathcal{L}(q,\\boldsymbol{\\theta})$ is a lower bound on $\\log p(\\mathbf{X}|\\boldsymbol{\\theta})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show that the EM algorithm does maximise log likelihood.\n",
    "\n",
    "Suppose that the current value of the parameter vector is $\\boldsymbol{\\theta}^{\\mathrm{old}}$. In the E step, the lower bound $\\mathcal{L}(q,\\boldsymbol{\\theta}^{\\mathrm{old}})$ is maximised with respect to $q(\\mathbf{Z})$ while holding $\\boldsymbol{\\theta}^{\\mathrm{old}}$ fixed. \n",
    "Writing it out explicitly,\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(q, \\boldsymbol{\\theta}^{\\mathrm{old}}) = -\\mathrm{KL}(q(\\mathbf{Z})||p(\\mathbf{Z}|\\mathbf{X},\\boldsymbol{\\theta}^{\\mathrm{old}})) + \\log p(\\mathbf{X}|\\boldsymbol{\\theta}^{\\mathrm{old}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution to this maximization problem is easily seen by noting that the value of $\\log p(\\mathbf{X}|\\boldsymbol{\\theta}^{\\mathrm{old}})$ does not depend on $q(\\mathbf{Z})$ and so the largest value of $\\mathcal{L}(q, \\boldsymbol{\\theta}^{\\mathrm{old}})$ will occur when the KL divergence vanishes, i.e. $q(\\mathbf{Z})$ equals the posterior distribution $p(\\mathbf{Z}|\\mathbf{X},\\boldsymbol{\\theta}^{\\mathrm{old}})$. This is the output of the E step.\n",
    "\n",
    "In the subsequent M step, the distribution $q(\\mathbf{Z})$ is held fixed and the lower bound $\\mathcal{L}(q,\\boldsymbol{\\theta})$ is maximized with respect to $\\boldsymbol{\\theta}$ to give some new value $\\boldsymbol{\\theta}^{\\mathrm{new}}$. This will cause the lower bound $\\mathcal{L}$ to increase, which will necessarily cause the corresponding log likelihood function to increase. Because the distribution $q$ is determined using the old parameter values rather than the new values and is held fixed during the M step, it will not equal the new posterior distribution $p(\\mathbf{Z}|\\mathbf{X},\\boldsymbol{\\theta}^{\\mathrm{new}})$, and hence there will be a nonzero KL divergence. The increase in the log likelihood function is therefore greater than the increase in the lower bound. If we substitute $q(\\mathbf{Z}) = p(\\mathbf{Z}|\\mathbf{X},\\boldsymbol{\\theta}^{\\mathrm{old}})$, we see that after the E step, the lower bound takes the form\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathcal{L}(q, \\boldsymbol{\\theta}) & = & \\sum_{\\mathbf{Z}}p(\\mathbf{Z}|\\mathbf{X},\\boldsymbol{\\theta}^{\\mathrm{old}})\\log p(\\mathbf{X},\\mathbf{Z}|\\boldsymbol{\\theta})-\\sum_{\\mathbf{Z}}p(\\mathbf{Z}|\\mathbf{X},\\boldsymbol{\\theta}^{\\mathrm{old}})\\log p(\\mathbf{Z}|\\mathbf{X},\\boldsymbol{\\theta}^{\\mathrm{old}}) \\\\\n",
    "& = & \\mathcal{Q}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{\\mathrm{old}}) + \\mathrm{const}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the constant is simply the negative entropy of the $q$ distribution and is therefore independent of $\\boldsymbol{\\theta}$. Thus in the M step, the quantity that is being maximized is the expectation of the complete data log likelihood, as we saw earlier in the case of mixture of Gaussians. \n",
    "\n",
    "We see that the lower bound is tight after the E step. Since the lower bound 'touches' the function, maximizing the lower bound will also 'push up' on the function itself. That is, the M step is guaranteed to modify the parameters so as to increase the likelihood of the observed data.\n",
    "\n",
    "Also,\n",
    "$$\n",
    "p(\\mathbf{X}|\\boldsymbol{\\theta}^{\\mathrm{new}}) \\ge \\mathcal{Q}(\\boldsymbol{\\theta}^{\\mathrm{new}},\\boldsymbol{\\theta}^{\\mathrm{old}}) \\ge \\mathcal{Q}(\\boldsymbol{\\theta}^{\\mathrm{old}},\\boldsymbol{\\theta}^{\\mathrm{old}}) = p(\\mathbf{X}|\\boldsymbol{\\theta}^{\\mathrm{old}})\n",
    "$$\n",
    "showing that EM monotonically increases the observed data log likelihood until it reaches a local optimum."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
