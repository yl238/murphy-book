{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Variational Inference for Text Processing\n",
    "In this notebook we'll be looking at a neural network alternative to LDA.\n",
    "\n",
    "Information from [Miao et al. 2016](https://arxiv.org/pdf/1511.06038.pdf)\n",
    "\n",
    "The basic idea of VI is simple: we want to estimate the true posterior $p(\\mathbf{X}|\\mathcal{D})$ which is in general intractable. What we do is to pick an approximation $q(\\mathbf{X})$ from some tractable family, and then to try to make this approximation as close as possible to the true posterior, usually by minimising the KL divergence from $p$ to $q$. This reduces inference to an optimization problem.\n",
    "\n",
    "Neural variational inference learns to model the posterior probability, thus endowing the model with strong generalisation abilities. By using the re-parameterisation method (Rezende et al. 2014), the inference network is trained through back-propagation unbiased and low variance gradients wrt the latent variables. Within this framework, Miao et al. proposes a Neural Variation Document Model (NVDM) for document modelling. \n",
    "\n",
    "<img src='figures/NVDM.png' width='30%'>\n",
    "\n",
    "NVDM is an unsupervised generative model of text which aims to extract a continuous semantic latent variable for each document. This model can be interpreted as a variational auto-encoder: a multi-layer perceptron (MLP) encoder (inference network) compresses the bag of words document representation into a continuous latent distribution, and a softmax decoder (generative model) reconstructs the document by generating the words independently.\n",
    "\n",
    "A primary feature of NVDM is that each word is generated directly from a **dense continuous document representation** instead of the more common **binary semantic vector**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Variational Inference Framework\n",
    "- We define a generative model with a latent variable $\\boldsymbol{h}$ which can be considered as the stochastic units in deep neural networks.\n",
    "- We designed the observed parent and child nodes of $\\boldsymbol{x}$ and $\\boldsymbol{y}$.\n",
    "- The joint distribution of the generative model is \n",
    "    $$p_\\theta(\\boldsymbol{x}, \\boldsymbol{y}) = \\sum_{\\boldsymbol{h}}p_\\theta(\\boldsymbol{y}|\\boldsymbol{h})p_\\theta(\\boldsymbol{h}|\\boldsymbol{x})p(\\boldsymbol{x})$$\n",
    "    \n",
    "The variational lower bound $\\mathcal{L}$ is derived as:\n",
    "$$\n",
    "\\begin{array}\n",
    "\\mathcal{L} & = \\mathbb{E}_{q(\\boldsymbol{h})}[\\log p_\\theta(\\boldsymbol{y}|\\boldsymbol{h})p_\\theta(\\boldsymbol{h}|\\boldsymbol{x})p(\\boldsymbol{x}) - \\log q(\\boldsymbol{h})] \\\\\n",
    " & \\le \\log\\int \\frac{q(\\boldsymbol{h})}{q(\\boldsymbol{h})} p_\\theta(\\boldsymbol{y}|\\boldsymbol{h})p_\\theta(\\boldsymbol{h}|\\boldsymbol{x})p(\\boldsymbol{x})d\\boldsymbol{h} = \\log p_\\theta(\\boldsymbol{x}, \\boldsymbol{y})\n",
    "\\end{array}\n",
    "$$\n",
    "(It's the difference between the true posterior and the variational approximation $q$)\n",
    "\n",
    "- $\\theta$ parameterises the generative distribution $p_\\theta(\\boldsymbol{y}|\\boldsymbol{h})$ and $p_\\theta(\\boldsymbol{h}|\\boldsymbol{x})$\n",
    "- Employ a parameterized diagonal Gaussian $\\mathcal{N}(\\boldsymbol{h}|\\boldsymbol{\\mu}(\\boldsymbol{x}, \\boldsymbol{y}), \\mathrm{diag}(\\boldsymbol{\\sigma}^2(\\boldsymbol{x}, \\boldsymbol{y})))$ as $q_\\theta(\\boldsymbol{h}|\\boldsymbol{x}, \\boldsymbol{y})$.\n",
    "\n",
    "### 3-steps to construct the inference network\n",
    "1. Construct vector representations of the observed variables: $\\boldsymbol{u} = f_x(\\boldsymbol{x}), \\boldsymbol{v} = f_y(\\boldsymbol{y})$.\n",
    "2. Assemble a joint representation: $\\boldsymbol{\\pi} = g(\\boldsymbol{u}, \\boldsymbol{v})$.\n",
    "3. Parameterise the variational distribution over the latent variable: $\\boldsymbol{\\mu} = l_1(\\boldsymbol{\\pi})$, $\\log\\boldsymbol{\\sigma} = l_2(\\boldsymbol{\\pi})$.\n",
    "\n",
    "$f_x(\\cdot)$ and $f_y(\\cdot)$ can be any type of deep neural networks that are suitable for observed data; $g(\\cdot)$ is an MLP that concatenates the vector representations of the conditioning variables; $l(\\cdot)$ is a linear transformation which outputs the parameters of the Gaussian distribution. By sampling from the variational distribution $\\boldsymbol{h}\\sim q_\\phi(\\boldsymbol{h}|\\boldsymbol{x}, \\boldsymbol{y})$, we are able to carry out stochastic back-propagation to optimise the lower bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Variational Document Model\n",
    "- Simple instance of unsupervised learning \n",
    "- A continuous hidden variable $\\boldsymbol{h}\\in \\mathbb{R}^K$, which generates all the words in a document independently, is introduced to represent is semantic content.\n",
    "- Let $\\boldsymbol{X} \\in \\mathbb{R}^{|V|}$ be the bag-of-words representation of a document and $\\boldsymbol{x}_i\\in\\mathbb{R}^{|V|}$ be the one-hot representation of the word at position $i$.\n",
    "\n",
    "Interpret NVDM as a variational autoencoder: \n",
    "- An MLP encoder $q(\\boldsymbol{h}|\\boldsymbol{X})$ compresses document representations into continuous hidden vectors ($\\boldsymbol{X}\\rightarrow\\boldsymbol{h}$)\n",
    "- A softmax decoder\n",
    "$$\n",
    "p(\\boldsymbol{X}|\\boldsymbol{h}) = \\prod_{i=1}^N p(\\boldsymbol{x}_i|\\boldsymbol{h})$$ reconstructs the documents by independently generating the words $(\\boldsymbol{h}\\rightarrow \\{\\boldsymbol{x}_i\\})$. \n",
    "- To maximise the log-likelihood $\\log\\sum_{\\boldsymbol{h}}p(\\boldsymbol{X}|\\boldsymbol{h})p(\\boldsymbol{h})$ of documents, we derive the lower bound:\n",
    "$$\n",
    "\\mathcal{L} = \\mathbb{E}_{q_\\phi(\\boldsymbol{h}|\\boldsymbol{X})}\\left[\\sum_{i=1}^N\\log p_\\theta(\\boldsymbol{x}_i|\\boldsymbol{h})\\right]-\\mathit{D}_{KL}[q_\\phi(\\boldsymbol{h}|\\boldsymbol{X})||p(\\boldsymbol{h})]\n",
    "$$\n",
    "where $N$ is the number of words in the document and $p(\\boldsymbol{h})$ is a Gaussian prior for $\\boldsymbol{h}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
