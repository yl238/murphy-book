{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Variational Inference\n",
    "Notes from the Hoffmann *et al.* (2013) paper "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In probabilistic modelling, we use hidden variables to encode hidden structure in observed data; we articulate the relationship between the hidden and observed variables with a factorized probability distribution (i.e. a graphical model) and we use inference algorithms to estimate the **posterior distribution**, the **conditional distribution** of hidden structure given the observations.\n",
    "\n",
    "Consider a graphical model of hidden and observed random variables for which we want to compute the posterior. For many models of interest, this posterior is not tractable to compute and we must appeal to approximate methods. The two most prominent strategies in statistics and machine learning are Markov chain Monte Carlo (MCMC) sampling and variational inference\n",
    "\n",
    "* **MCMC Sampling**:\n",
    "    We construct a Markov chain over the hidden variables whose stationary distribution is the posterior of interest. We run the chain until it has (hopefully) reached equilibrium and collect samples to approximate the posterior.\n",
    "* **Variational inference**:\n",
    "    We define a flexible family of distribution over the hidden variables, indexed by free parameters. We then find the setting of the parameters (i.e. the member of the family) that is closest to the posterior. Thus we solve the inference problem by solving an optimization problem.\n",
    "    \n",
    "The aim here is to develop a general variational method that scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Form of stochastic variational inference:\n",
    "1. Subsample one or more data points from the data\n",
    "2. Analyze the subsample using the current variational parameters\n",
    "3. Implement a closed-form update of the variational parameters.\n",
    "4. Repeat.\n",
    "While traditional algorithms require repeatedly analyzing the whole dataset before updating the variational parameters, this algorithm only requires that we analyze randomly sampled subsets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVI is a sochastic **optimization algorithm** for mean-field variational inference. It approximates the posterior distribution of a probabilistic model with hidden variables, and can handle massive data sets of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![graphical model](figures/classic.png)\n",
    "A graphical model with observations $x_{1:N}$, local hidden variables $z_{1:N}$ and global hidden variables $\\beta$. The distribution of each observation $x_n$ only depends on its corresponding local variable $z_n$ and the global variables $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the class of models to which our algorithm applies. \n",
    "We define *local* and *global* hidden variables, and requirements on the conditional distributions within the model.\n",
    "\n",
    "The joint distribution factorizes into a global term and a product of local terms:\n",
    "$$\n",
    "p(x, z, \\beta |\\alpha) = p(\\beta | \\alpha)\\prod_{n=1}^N p(x_n, z_n |\\beta)\n",
    "$$\n",
    "Our goal is to approximate the posterior distribution of the hidden variables given the observations, $p(\\beta, z|x)$.\n",
    "\n",
    "**Assumption 1**: The $n$th observation $x_n$ and the $n$th local variable $z_n$ are conditionally independent, given global variables $\\beta$, of all other observations and local hidden variables,\n",
    "$$\n",
    "p(x_n, z_n |x_{-n}, z_{-n},\\beta,\\alpha) = p(x_n, z_n|\\beta,\\alpha)\n",
    "$$\n",
    "\n",
    "**Assumption 2**: The *complete conditionals* in the model. A complete conditional is the conditional distribution of a hidden variable given the other hidden variables and the observations. We assume that these distributions are in the **exponential family**,\n",
    "\n",
    "\\begin{eqnarray}\n",
    "p(\\beta|x, z, \\alpha) & = & h(\\beta)\\exp\\{\\eta_g(x, z, \\alpha)^Tt(\\beta)-a_g(\\eta_g(x, z, \\alpha))\\}\\\\\n",
    "p(z_{nj}|x_n, z_{n,-j}, \\beta) & = & h(z_{nj})\\exp\\{\\eta_l(x_n, z_{n,-j},\\beta)^Tt(z_{nj}) - a_l(\\eta_l(x_n, z_{n,-j},\\beta))\\}\n",
    "\\end{eqnarray}\n",
    "\n",
    "The scala functions $h(\\cdot)$ and $a(\\cdot)$ are respectively the *base measure* and *log-normalizer*, the vector functions $\\eta(\\cdot)$ and $t(\\cdot)$ are respectively the *natural parameter* and *sufficient statistics*. **(For details of this consult a basic statistic book on exponential distributions)**. \n",
    "\n",
    "These are conditional distributions, so the natural parameter is a function of the variables that are being conditioned on. For the local variables $z_{nj}$, the complete conditional distribution is determined by the global variables $\\beta$ and the other local variables in the $n$th context, i.e. the $n$th data point $x_n$ and the local variables $z_{n,-j}$.\n",
    "\n",
    "These assumptions on the complete conditional imply a **conjugacy relationship** between the global variables $\\beta$ and the local contexts $(z_n, x_n)$, and this relationship implies the distribution of the local context given the global variables must be in an exponential family,\n",
    "\n",
    "\\begin{equation}\n",
    "p(x_n, z_n |\\beta) = h(x_n, z_n)\\exp\\{\\beta^Tt(x_n, z_n) - a_l(\\beta)\\}\n",
    "\\end{equation}\n",
    "\n",
    "The prior distribution $p(\\beta)$ must also be in an exponential family,\n",
    "$$\n",
    "p(\\beta) = h(\\beta)\\exp\\{\\alpha^Tt(\\beta)-a_g(\\alpha)\\}\n",
    "$$\n",
    "The sufficient statistics are $t(\\beta) = (\\beta, -a_l(\\beta))$ and thus the hyperparameter $\\alpha$ has two components $\\alpha = (\\alpha_1, \\alpha_2)$. The first component $\\alpha_1$ is a vector of the same dimension as $\\beta$, the second component $\\alpha_2$ is a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two equations above imply that the complete conditional for the global variable is in the same exponential family as the prior with natural parameter\n",
    "\n",
    "$$\n",
    "\\eta_g(x, z, \\alpha) = (\\alpha_1 + \\sum_{n=1}^N t(z_n, x_n), \\alpha_2+N).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysing data with one of the model associated with this family of distributions (e.g. Bayesian mixture models, Latent Dirichlet allocation) amounts to computing the posterior distribution of the hidden variables given the observations,\n",
    "\n",
    "$$\n",
    "p(z, \\beta |x ) = \\frac{p(x, z, \\beta)}{\\int p(x, z, \\beta)dz d\\beta}.\n",
    "$$\n",
    "We then use this posterior to explore the hidden structure of our data or to make predictions about future data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mean field variational inference\n",
    "An approximate inference strategy that seeks a tractable distribution over the hidden variables which is close to the posterior distribution. Derive the traditional variational inference algorithm for our class of models, which is a coordinate ascent algorithm. Closeness is measured with the KL divergence. We use the resulting distribution, called the *variational distribution* to approximate the posterior.\n",
    "\n",
    "### The evidence lower bound\n",
    "Variational inference minimizes the KL divergence from the variational distribution to the posterior distribution. It maximizes the *evidence lower bound* (ELBO), a lower bound on the logarithm of the marginal probability of the observations $\\log p(x)$. The ELBO is equal to the negative KL divergence up to an additive constant.\n",
    "\n",
    "We derive the ELBO by introducing a distribution over the hidden variables $q(\\alpha, \\beta)$ and using Jensen's inequality. (This implies $\\log\\mathbb{E}[f(y)]\\ge \\mathbb{E}[\\log f(y)]$ for any random variable $y$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives the following bound on the log marginal,\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\log p(x) & = & \\log\\int p(x, z, \\beta)dz d\\beta\\\\\n",
    "& = & \\log\\int p(x, z, \\beta)\\frac{q(z, \\beta)}{q(z, \\beta)}dzd\\beta\\\\\n",
    "& = & \\log\\left(\\mathbb{E}_q\\left[\\frac{p(x,z,\\beta)}{q(z,\\beta)}\\right]\\right)\\\\\n",
    "&\\ge & \\mathbb{E}_q[\\log p(x, z, \\beta)]-\\mathbb{E}[\\log q(z, \\beta)]\\\\\n",
    "&\\triangleq &\\mathcal{L}(q).\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ELBO contains two terms. The first term is the expected log joint, $\\mathbb{E}_q[\\log p(x, z, \\beta)]$. The second is the entropy of the variational distribution, $-\\mathbb{E}_q[\\log q(z, \\beta)]$. Both of these terms depend on $q(z, \\beta)$, the variational distribution of the hidden variables.\n",
    "\n",
    "We restrict $q(z, \\beta)$ to be in a family that is tractable, one for which the expectations in the ELBO can be efficiently computed. We then try to find the member of the family that maximizes the ELBO. Finally, we use the optimized distribution as a proxy for the posterior.\n",
    "\n",
    "Solving this maximization problem is equivalent to finding the member of the family that is closest in KL divergence to the posterior:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "KL(q(z,\\beta)||p(z, \\beta|x)) & = & \\mathbb{E}_q[\\log q(z, \\beta)] - \\mathbb{E}_q[\\log p(z, \\beta|x)]\\\\\n",
    "& = & \\mathbb{E}_q[\\log q(z, \\beta)]-\\mathbb{E}_q[\\log p(x, \\, \\beta)] + \\log p(x)\\\\\n",
    "& = & -\\mathcal{L}(q) + \\mathrm{const}.\n",
    "\\end{eqnarray}\n",
    "$\\log p(x)$ is replaced by a constant because it does not depend on $q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The mean-field variational family.\n",
    "The simplest variational family of distributions. In this family, each hidden variable is independent and governed by its own parameter,\n",
    "\n",
    "$$\n",
    "q(z, \\beta) = q(\\beta |\\lambda)\\prod_{n=1}^N \\prod_{j=1}^J q(z_{nj}|\\phi_{nj})\n",
    "$$\n",
    "\n",
    "The global parameters $\\lambda$ govern the global variables, the local parameters $\\phi_n$ govern the local variables in the $n$th context. The ELBO is a function of these parameters.\n",
    "\n",
    "We set $q(\\beta|\\lambda)$ and $q(z_{nj}|\\phi_{nj})$ to be in the same exponential family as the complete conditional distributions $p(\\beta|x, z)$ and $p(z_{nj}|x_n,z_{n,-j},\\beta)$. The variational parameters $\\lambda$ and $\\phi_{nj}$ are the natural parameters to those families,\n",
    "\n",
    "\\begin{eqnarray}\n",
    "q(\\beta|\\lambda) & = & h(\\beta)\\exp\\{\\lambda^Tt(\\beta)-a_g(\\lambda)\\}\\\\\n",
    "q(z_{nj}|\\phi_{nj}) & = & h(z_{nj})\\exp\\{\\phi_{nj}^Tt(z_{nj}) - a_{l}(\\phi_{nj})\\}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
