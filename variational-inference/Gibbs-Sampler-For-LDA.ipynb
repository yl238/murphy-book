{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gibbs Sampler for LDA\n",
    "\n",
    "Latent Dirichlet Allocation (LDA) is a mixed membership model for topic modelling. Given a set of documents in a bag of words representation, we want to infer the underlying topics those documents represent. To get a better intuition, we shall look at LDA's generative story. \n",
    "\n",
    "Given $i = \\{1,\\ldots,N_D\\}$ the document index, $v = \\{1,\\ldots,N_W\\}$ the word index, $k= \\{1,\\ldots,N_k\\}$ the topic index, LDA assumes:\n",
    "\n",
    "$$\n",
    "\\begin{array}\n",
    "& \\pi_i &\\sim & \\mathrm{Dir}(\\pi_i|\\alpha)\\\\\n",
    "z_{iw} & \\sim &\\mathrm{Cat}(z_{iw}|\\pi_i)\\\\\n",
    "\\mathbf{b}_k &\\sim & \\mathrm{Dir}(\\mathbf{b}_k|\\gamma)\\\\\n",
    "y_{iw} &\\sim &\\mathrm{Cat}(y_{iw}|z_{iw} = k, \\mathbf{B})\n",
    "\\end{array}\n",
    "$$\n",
    "where $\\alpha$ and $\\gamma$ are the parameters for the Dirichlet priors. They tell us how narrow or spread the document topic and topic word distributions are.\n",
    "\n",
    "Details for the above generative process in words:\n",
    "1. Assume each document is generated by selecting the topic first. Thus, sample $\\pi_i$, the topic distribution for the $i$-th document.\n",
    "2. Assume each word in the $i$-th document comes from one of the topics. Therefore, we sample $z_{iw}$, the topic for each word $w$ in document $i$.\n",
    "3. Assume each topic is composed of words, e.g. topic 'computer' consists of words 'cpu', 'gpu', etc. Therefore, we sample $\\mathbf{b}_k$, the distribution of those words for particular topic $k$.\n",
    "4. Finally, to actually generate the word, given that we already know it comes from topic $k$, we sample the word $y_{iw}$ given the $k$-th topic word distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "The goal of inference in LDA is that given a corpus, we infer the underlying topics that explain those documents, according to the generative process above. Essentially, given $y_{iw}$, we are inverting the above process to find $z_{iw}$, $\\pi_i$ and $\\mathbf{b}_k$.\n",
    "\n",
    "We will infer those variables using Gibbs Sampling algorithm. In short, it works by sampling each of those variables given the other variables (full conditional distribution). Because of the conjugacy, the full conditionals are as follows:\n",
    "\n",
    "$$\n",
    "\\begin{array}\n",
    "& p(z_{iw} = k|\\pi_i, \\mathbf{b}_k) &\\propto & \\exp(\\log \\pi_{ik} + \\log b_{k, y_{iw}}) \\\\\n",
    "p(\\pi_i | z_{iw} = k, \\mathbf{b}_k) & = & \\mathrm{Dir}(\\alpha + \\sum_l\\mathbb{I}(z_{il} = k )) \\\\\n",
    "p(\\mathbf{b}_k|z_{iw} = k, \\pi_i) & = & \\mathrm{Dir}(\\gamma + \\sum_i\\sum_l\\mathbb{I}(y_{il}=w, z_{il}=k))\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Essentially, what we are doing is to count the assignment of words and documents to particular topics. Those are the sufficient statistics for the full conditionals.\n",
    "\n",
    "Given those full conditionals, the rest is as easy as plugging those into the Gibbs Sampling framework, as we shall discuss in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words\n",
    "W = np.array([0, 1, 2, 3, 4])\n",
    "\n",
    "# D:= document words\n",
    "X = np.array([\n",
    "    [0, 0, 1, 2, 2],\n",
    "    [0, 0, 1, 1, 1],\n",
    "    [0, 1, 2, 2, 2],\n",
    "    [4, 4, 4, 4, 4],\n",
    "    [3, 3, 4, 4, 4],\n",
    "    [3, 4, 4, 4, 4]\n",
    "])\n",
    "\n",
    "N_D = X.shape[0] # num of docs\n",
    "N_W = W.shape[0] # num of words\n",
    "N_K = 2 # num of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with randomly initializing topic assignment matrix $\\mathbf{Z}_{N_D\\times N_W}$. We also sample the initial values of $\\boldsymbol{\\Pi}_{N_D\\times N_K}$ and $\\mathbf{B}_{N_K\\times N_W}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dirichlet priors\n",
    "alpha = 1\n",
    "gamma = 1\n",
    "\n",
    "# --------------\n",
    "# Initialization\n",
    "# --------------\n",
    "\n",
    "# Z := word topic assigmnet\n",
    "Z = np.zeros(shape=[N_D, N_W])\n",
    "for i in range(N_D):\n",
    "    for l in range(N_W):\n",
    "        Z[i, l] = np.random.randint(N_K) # randomly assign word's topic\n",
    "        \n",
    "# Pi := document topic distribution\n",
    "Pi = np.zeros([N_D, N_K])\n",
    "for i in range(N_D):\n",
    "    Pi[i] = np.random.dirichlet(alpha*np.ones(N_K))\n",
    "    \n",
    "# B := word topic distribution\n",
    "B = np.zeros([N_K, N_W])\n",
    "for k in range(N_K):\n",
    "    B[k] = np.random.dirichlet(gamma*np.ones(N_W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sample the new values for each of those variables from the full conditionals in the previous section and iterate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------\n",
    "# Gibbs sampling\n",
    "# --------------\n",
    "for it in range(1000):\n",
    "    # Sample from full conditional of Z\n",
    "    # ---------------------------------\n",
    "    for i in range(N_D):\n",
    "        for l in range(N_W):\n",
    "            # Calculate params for Z\n",
    "            p_bar_il = np.exp(np.log(Pi[i]) + np.log(B[:, X[i, l]]))\n",
    "            p_il = p_bar_il / np.sum(p_bar_il)\n",
    "            \n",
    "            # Resample word topic assignment Z\n",
    "            z_il = np.random.multinomial(1, p_il)\n",
    "            Z[i, l] = np.argmax(z_il)\n",
    "            \n",
    "    # Sample from full conditional of Pi\n",
    "    # ----------------------------------\n",
    "    for i in range(N_D):\n",
    "        m = np.zeros(N_K)\n",
    "        \n",
    "        # Gather sufficient statistics\n",
    "        for k in range(N_K):\n",
    "            m[k] = np.sum(Z[i] == k)\n",
    "            \n",
    "        # Resample doc topic distribution.\n",
    "        Pi[i, :] = np.random.dirichlet(alpha + m)\n",
    "        \n",
    "    # Sample from full conditional of B\n",
    "    # ---------------------------------\n",
    "    for k in range(N_K):\n",
    "        n = np.zeros(N_W)\n",
    "        \n",
    "        # Gather sufficient statistics\n",
    "        for v in range(N_W):\n",
    "            for i in range(N_D):\n",
    "                for l in range(N_W):\n",
    "                    n[v] += (X[i, l] == v) and (Z[i, l] == k)\n",
    "                    \n",
    "        # Resample word topic distribution\n",
    "        B[k, :] = np.random.dirichlet(gamma + n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And basically we are done. We could inspect the result by looking at those variables after some iterations of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents:\n",
      "----------\n",
      "[[0 0 1 2 2]\n",
      " [0 0 1 1 1]\n",
      " [0 1 2 2 2]\n",
      " [4 4 4 4 4]\n",
      " [3 3 4 4 4]\n",
      " [3 4 4 4 4]]\n"
     ]
    }
   ],
   "source": [
    "print('Documents:')\n",
    "print('----------')\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document topic distribution:\n",
      "----------------------------\n",
      "[[0.17379786 0.82620214]\n",
      " [0.33210815 0.66789185]\n",
      " [0.0322541  0.9677459 ]\n",
      " [0.75891264 0.24108736]\n",
      " [0.84967238 0.15032762]\n",
      " [0.81500144 0.18499856]]\n"
     ]
    }
   ],
   "source": [
    "print('Document topic distribution:')\n",
    "print('----------------------------')\n",
    "print(Pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic's word distribution:\n",
      "---------------------------\n",
      "[[0.11478112 0.07575037 0.03618259 0.06698306 0.70630286]\n",
      " [0.2382687  0.3871144  0.1468959  0.17844455 0.04927644]]\n"
     ]
    }
   ],
   "source": [
    "print('Topic\\'s word distribution:')\n",
    "print('---------------------------')\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word topic assignment:\n",
      "----------------------\n",
      "[[1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print('Word topic assignment:')\n",
    "print('----------------------')\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
