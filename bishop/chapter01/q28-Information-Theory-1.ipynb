{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Theory\n",
    "\n",
    "- Consider a discrete random variable $x$ and ask how much information is received when we observe a specific value for this variable.\n",
    "- The amount of information can be viewed as the 'degree of surprise' on learning the value of $x$. <br>\n",
    "    *Information*(highly improbable event) > *Info*(probable event) > *Info*(Certain event) = 0\n",
    "- Measure of information will depend on the probability distribution $p(x)$\n",
    "- We look for a quantity $h(x)$ that is a monotonic function of the probability $p(x)$ and that expresses the information content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving the form of $h(x)$\n",
    "Conditions:\n",
    "\n",
    "1. If we have two events $x$ and $y$ that are unrelated, then the information gain from observing both of them should be the sum of the information gained from each of them separately, so that $h(x, y) = h(x) + h(y)$.\n",
    "2. These two events are statistically independent and $p(x, y) = p(x)p(y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "h(x, y) = f(p(x, y)) = f(p(x)p(y)) = f(p(x)) + f(p(y))\n",
    "$$\n",
    "where $f$ is a monotonic function.\n",
    "\n",
    "Remember that $\\log(xy) = \\log(x) + \\log(y)$, so \n",
    "\n",
    "$$h(x) = -\\log p(x)).$$\n",
    "The negative sign ensures that information is positive or zero. As can be seen, low probability events $x$ corresponds to high information content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "\n",
    "Suppose that a sender wishes to transmit the value of a random variable to a receiver. The average amount of information that they transmit in the process is obtained by taking the expection of $h(x)$ with respect to the distribution $p(x)$ and is given by \n",
    "\n",
    "$$\n",
    "H[x] = -\\sum_xp(x)\\log p(x)\n",
    "$$\n",
    "\n",
    "This is called the *entropy* of the random variable $x$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
