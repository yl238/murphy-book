{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Inference\n",
    "\n",
    "## Probability Theory\n",
    "### Definitions\n",
    "\n",
    "- $p(\\mathbf{x})$ is the *marginal probability* of $\\mathbf{x}$\n",
    "- $p(\\mathbf{x}, \\mathbf{y})$ is the *joint* probability of $\\mathbf{x}$ and $\\mathbf{y}$\n",
    "- $p(\\mathbf{x}|\\mathbf{y})$ is the *conditional* probability of $\\mathbf{x}$ given $\\mathbf{y}$.\n",
    "\n",
    "### Rules of Probability\n",
    "- $0 < p(\\mathbf{x}) < 1$\n",
    "- Probabilities must sum to 1: $\\sum_{\\mathbf{x}}p(\\mathbf{x}) = 1$\n",
    "- Product rule: $p(\\mathbf{x}, \\mathbf{y}) = p(\\mathbf{x}|\\mathbf{y})p(\\mathbf{y}) = p(\\mathbf{y}|\\mathbf{x})p(\\mathbf{x})$\n",
    "- Sum rule: $p(\\mathbf{x}) = \\sum_{\\mathbf{y}}p(\\mathbf{x},\\mathbf{y})$\n",
    "\n",
    "Bayes' rule is derived from the product rule:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}|\\mathbf{y}) = \\frac{p(\\mathbf{y}|\\mathbf{x})p(\\mathbf{x})}{p(\\mathbf{y})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have observed data $\\mathcal{D} = \\{x_1,\\ldots,x_N\\}$ generated from a model with parameters $\\mathbf{w}$. We can capture our assumptions about $\\mathbf{w}$, before observing data, in the form of a prior probability distribution $p(\\mathbf{w}$. The effect of the observed data is expressed through the conditional probability $p(\\mathcal{D}|\\mathbf{w})$. \n",
    "\n",
    "Bayes' theorem, which takes the form \n",
    "\n",
    "$$\n",
    "p(\\mathbf{w}|\\mathcal{D}) = \\frac{p(\\mathcal{D}|\\mathbf{w})p(\\mathbf{w})}{p(\\mathcal{D})}\n",
    "$$\n",
    "\n",
    "then allows us to evaluate the uncertainty in $\\mathbf{w}$ *after* we have observed $\\mathcal{D}$ in the form of the posterior probability $p(\\mathbf{w}|\\mathcal{D})$.\n",
    "\n",
    "The quantity $p(\\mathcal{D}|\\mathbf{w})$ on the right-hand side of Bayes' theorem is evaluated for the observed dataset $\\mathcal{D}$ and can be viewed a function of the parameter vector $\\mathbf{w}$, in which case it is called the *likelihood function*. It expresses how probable the observed dataset is for different settings of the parameter vector $\\mathbf{w}$. Note that the likelihood is not a probability distribution over $\\mathbf{w}$, and its integral with respect to $\\mathbf{w}$ does not (necessarily) equal one.\n",
    "\n",
    "Given this definition of likelihood, we can state Baye's theorem in words as \n",
    "\n",
    "$$\n",
    "\\mathrm{posterior}\\propto\\mathrm{likelihood}\\times\\mathrm{prior}\n",
    "$$\n",
    "\n",
    "where all of these quantities are viewed as functions of $\\mathbf{w}$. The denominator in Bayes' theorem can be expressed in terms of the prior distribution and the likelihood function\n",
    "\n",
    "$$\n",
    "p(\\mathcal{D}) = \\int p(\\mathcal{D}|\\mathbf{w})p(\\mathbf{w})d\\mathbf{w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic (Supervised) Learning\n",
    "\n",
    "We have a dataset consisting of input/output pairs:\n",
    "\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "\\mathcal{D} & = \\{\\mathbf{x}_i, \\mathbf{y}_i\\}_{i=1}^n & \\\\\n",
    "\\mathbf{X} & = [\\mathbf{x}_1,\\ldots,\\mathbf{x}_n]^T & \\\\\n",
    "\\mathbf{y} & = [y_1,\\ldots,y_n]^T & \\mathrm{binary/regression} \\\\\n",
    "\\mathbf{Y} & = [\\mathbf{y}_1^T,\\ldots,\\mathbf{y}^T_n] & \\mathrm{multiclass}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "\\mathbf{w} &= [\\mathbf{w}_1,\\ldots,\\mathbf{w}_C]^T & \\mathrm{parameters\\, (weights)} \\\\\n",
    "\\sigma & = [\\sigma_1,\\ldots,\\sigma_q]^T & \\mathrm{likelihood\\, hyperparameters} \\\\\n",
    "\\theta & = [\\theta_1,\\ldots,\\theta_p]^T & \\mathrm{prior\\, hyperparameters}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a probabilistic model, we start with choosing the **likelihood** function which describes how the data were produced\n",
    "\n",
    "$$\n",
    "p(\\mathrm{data}|\\mathrm{parameters}) = p(\\mathbf{y}|\\mathbf{w},\\mathbf{X}, \\sigma)\n",
    "$$\n",
    "\n",
    "There are many possible choices, depending on our problem, e.g. if we are performing regression or classification.\n",
    "\n",
    "We also specify our **prior** beliefs about the weight vector\n",
    "\n",
    "$$\n",
    "p(\\mathrm{parameters}|\\mathrm{model}) = p(\\mathbf{w}|\\theta)\n",
    "$$\n",
    "\n",
    "You can think of this as similar to regularization in non-probabilistic approaches.\n",
    "\n",
    "Inference then amounts to computing the posterior distribution (Bayes rule)\n",
    "\n",
    "<img src=\"prob_inference.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The **posterior** gives a distribution for the weight vector $\\mathbf{w}$ given the data. We then can use this to perform predictions.\n",
    "* The marginal likelihood enables us to perform model selection and choose the optimal values for the hyperparameters $\\theta$, $\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "The marginal likelihood (evidence) plays an important role in probabilistic modelling\n",
    "\n",
    "$$\n",
    "p(\\mathbf{y}|\\mathbf{X},\\theta,\\sigma) = \\int p(\\mathbf{y}|\\mathbf{X},\\mathbf{w}, \\sigma)p(\\mathbf{w}|\\theta)d\\mathbf{w}\n",
    "$$\n",
    "\n",
    "It embodies a tradeoff between data fit and model complexity and can be used for\n",
    "* Deciding which of several competeing models is most probable\n",
    "* Automatic optimisation of hyperparameters $\\theta, \\sigma$ by **evidence maximisation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Theory\n",
    "\n",
    "In probabilistic models, we commonly divide the learning process into two phases:\n",
    "1. **Inference**: computing the posterior distributions\n",
    "2. **Decision**: make a prediction / decision based on the posterior\n",
    "\n",
    "* Decision theory concerns the second step (e.g. given the class probabilities, should we choose treatment $A$ or $B$?)\n",
    "* This framework is highly flexible: e.g. we can accommodate asymmetric misclassification costs where a false negative may be more costly than a false positive (medical applications)\n",
    "\n",
    "In contrast, many approaches combine these two phases and learn a function that directly maps inputs $\\mathbf{x}$ onto class labels ($y$). This is called a *discriminant function* approach (e.g. SVM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function and Risk\n",
    "We can formalize the measurement of model performance using some \"loss function\" $\\mathcal{L}(y, f(\\mathbf{x}))$. There are many different loss functions for classification (e.g. classification error) and regression (e.g. MSE). \n",
    "\n",
    "The expected generalizability is then given by its \"Risk\":\n",
    "\n",
    "$$\n",
    "\\mathcal{R}[f] = \\int \\mathcal{L}(y, f(\\mathbf{x}))p(y,\\mathbf{x})dyd\\mathbf{x}\n",
    "$$\n",
    "\n",
    "However, we usually don't know $p(y,\\mathbf{x})$, so we approximate this by the \"empirical risk\", defined over the training set\n",
    "\n",
    "$$\n",
    "\\mathcal{R}_{emp}[f] = \\frac{1}{n}\\sum_{i=1}^n\\mathcal{L}(y, f(\\mathbf{x}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimising the empirical risk\n",
    "Consider a linear model that aims to predict the output ($y$) using a weighted combination of the inputs $\\mathbf{x}$\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x},\\mathbf{w}) = \\mathbf{x}^T\\mathbf{w} + b\n",
    "$$\n",
    "\n",
    "To estimate the weights, we seek to minimise the empirical risk, which is penalised to restrict model flexibility\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{w}} = \\min_{\\mathbf{w}}\\sum_{i=1}^n\\mathcal{L}(y_i,\\mathbf{x}_i,\\mathbf{w})+\\lambda J(\\mathbf{w})\n",
    "$$\n",
    "\n",
    "Many algorithms (e.g. SVM, Lasso, Ridge regression) are particular choices of $\\mathcal{L}()$ and $J()$.\n",
    "\n",
    "Probabilistic models can be viewed from a similar perspective: we want to minimise\n",
    "\n",
    "$$\n",
    "\\log p(\\mathbf{w}|\\mathbf{y}, \\mathbf{X},\\theta,\\sigma)\\propto \\sum_{i=1}^n\\log p(y_i|\\mathbf{w},\\mathbf{x}_i,\\sigma) + \\log p(\\mathbf{w}|\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic classification and regression\n",
    "* The discriminant function approach is appealing and is often very efficient\n",
    "* However, separating inference and decision also provides benefits, especially for classification\n",
    "\n",
    "### Advantages of probabilistic classification\n",
    "- Minimising risk (e.g. misclassification costs may change)\n",
    "- Compensate for class priors (accomodate disease prevalence)\n",
    "- Reject option (only make a decision if sufficiently confident)\n",
    "- Combining classifiers\n",
    "- Easily interpretable (predictive confidence)\n",
    "\n",
    "Coherent handling of uncertainty is especially important in medicine\n",
    "\n",
    "### Souces of uncertainty in clinical applications\n",
    "- Diagnostic uncertainty (class labels may be noisy)\n",
    "- Heterogeneity in disease severity and course\n",
    "- Individual variability in response to treatment\n",
    "\n",
    "In such applications, predictive confidence is potentially highly informative about individual variability.\n",
    "\n",
    "$p(y|\\mathbf{x}) = 0.55$: ambiguous, $p(y|\\mathbf{x})=0.99$: confident."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
