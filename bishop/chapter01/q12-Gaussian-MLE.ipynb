{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimator for Gaussian\n",
    "\n",
    "Suppose we have a dataset of observations $\\boldsymbol{\\mathsf{x}} = (x_1,\\ldots,x_N^T)$, representing $N$ observations of the scalar variable $x$. Note that we are using the typeface $\\boldsymbol{\\mathsf{x}}$ to distinguish this from a single observation of the vector-valued variable $(x_1,\\ldots,x_D)^T$, which we denote by $\\mathbf{x}$. \n",
    "\n",
    "We shall suppose that the observations are drawn independently from a Gaussian distribution whose mean $\\mu$ and variance $\\sigma^2$ are unknown, and we would like to determine these parameters from the dataset.\n",
    "\n",
    "Data points that are draw independently from the samedistribution are said to be *independent and identically distributed* (iid). We have seen that the joint probability of two independent events is given by the product of the marginal probabilities for each event separately. Because our dataset $\\boldsymbol{\\mathsf{x}}$ is i.i.d., we can therefore write the probability of the data set, given $\\mu$ and $\\sigma^2$, in the form\n",
    "\n",
    "$$\n",
    "p(\\boldsymbol{\\mathsf{x}}|\\mu,\\sigma^2) = \\prod_{n=1}^N\\mathcal{N}(x_n|\\mu, \\sigma^2)\n",
    "$$\n",
    "\n",
    "When viewed as a function of $\\mu$ and $\\sigma^2$, this is the **likelihood function** for the Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the moment, we shall determine values for the unknown parameters $\\mu$ and $\\sigma^2$ in the Gaussian by maximizing the likelihood function (or minimizing the log-likelihood function).\n",
    "\n",
    "The log-likelihood function can be written in the form\n",
    "\n",
    "$$\n",
    "\\mathrm{ln}p(\\boldsymbol{\\mathsf{x}}|\\mu,\\sigma^2) = -\\frac{1}{2\\sigma^2}\\sum_{n=1}^N(x_n-\\mu)^2 - \\frac{N}{2}\\mathrm{ln}\\sigma^2 - \\frac{N}{2}\\mathrm{ln}(2\\pi)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the derivative with respect to $\\mu$ and set it to zero, we have\n",
    "\n",
    "$$\\sum_{n=1}^N(x_n - \\mu_{ML}) = 0 \\implies \\mu_{ML} = \\frac{1}{N}\\sum_{i=1}^Nx_n,$$ where $\\mu_{ML}$ is the maximum likelihood solution, which is the *sample mean*.\n",
    "\n",
    "Similarly, the maximum likelihood solution for the variance can be found as \n",
    "\n",
    "\\begin{aligned}\n",
    "\\frac{1}{\\sigma^2}\\sum_{n=1}^N(x_n-\\mu)^2 -N = 0 \\\\\n",
    "\\sigma_{ML}^2 = \\frac{1}{N}\\sum_{n=1}^N(x_n-\\mu_{ML})^2\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE approach systematically underestimates the variance of the distribution:\n",
    "\n",
    "First, we note that the maximum likelihood solutions $\\mu_{ML}$ and $\\sigma^2_{ML}$ are functions of the dataset $x_1,\\ldots,x_n$. Consider the expectations of these quantities with respect to the dataset values, which themselves come from a Gaussian distribution with parameters $\\mu$ and $\\sigma^2$.\n",
    "\n",
    "So \n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\mu_{ML}] = \\frac{1}{N}\\sum_{i=1}^N\\mathbb{E}[x_n] = \\mu\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.12 \n",
    "Using the results (1.49) and (1.50), show that\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[x_n x_m] = \\mu^2 + I_{nm}\\sigma^2\n",
    "$$\n",
    "(1.130)\n",
    "\n",
    "where $x_n$ and $x_m$ denote data points sampled from a Gaussian distribution with mean $\\mu$ and variance $\\sigma^2$, and $I_{nm}$ satisfies $I_{nm} = 1$ if $n = m$ and $I_{nm} = 0$ otherwise.\n",
    "Hence prove the results (1.57) and (1.58)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) If $n \\ne m$, then $x_n$ and $x_m$ are iid and so $\\mathbb{E}[x_n x_m] = \\mathbb{E}[x_n]\\mathbb{E}[x_m] = \\mu^2$\n",
    "\n",
    "(b) If $n = m$, then (1.50) shows that $\\mathbb{E}[x_n^2]  = \\mu^2 + \\sigma^2$. Combining these together gives the required result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Prove that \n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\sigma_{ML}^2] = \\left(\\frac{N-1}{N}\\right)\\sigma^2\n",
    "$$\n",
    "\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}[\\sigma_{ML}^2] & = \\mathbb{E}\\left[\\frac{1}{N}\\sum_{n=1}^N(x_n - \\frac{1}{N}\\sum_{m=1}^Nx_m)^2\\right]\\\\\n",
    "& = \\frac{1}{N}\\sum_{n=1}^N\\mathbb{E}\\left[x_n^2 - \\frac{2}{N}x_n\\sum_{m=1}^Nx_m + \\frac{1}{N^2}\\sum_{m=1}^N\\sum_{l=1}^Nx_m x_l\\right] \\\\\n",
    "& = \\left\\{\\mu^2 + \\sigma^2 -2 \\left(\\mu^2+\\frac{1}{N}\\sigma^2\\right)+\\mu^2+\\frac{1}{N}\\sigma^2\\right\\}\\\\\n",
    "& = \\left(\\frac{N-1}{N}\\right)\\sigma^2\n",
    " \\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "What this means is that on average, the maximum likelihood estimate will obtain the correct mean but will underestimate the true variance by a factor $(N-1)/N$. \n",
    "\n",
    "From the above it follows the following estimate for the variance parameter is unbiased:\n",
    "\n",
    "$$\n",
    "\\tilde{\\sigma}^2 = \\frac{N}{N-1}\\sigma_{ML}^2 = \\frac{1}{N-1}\\sum_{n=1}^N(x_n - \\mu_{ML})^2.\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
