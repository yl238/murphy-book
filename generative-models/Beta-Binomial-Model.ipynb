{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The beta-binomial model\n",
    "Consider the problem of inferring the probability that a coin shows up heads, given a series of observed coin tosses. This model forms the basis of many methods including naive Bayes classifiers, Markov Models etc. Let's specify the likelihood and prior, and deriving the posterior and posterior predictive.\n",
    "\n",
    "### Likelihood\n",
    "Suppose $X_i\\sim \\mathrm{Ber}(\\theta)$, where $X_i = 1$ represents \"heads\", $X_i=0$ represents \"tails\", and $\\theta\\in [0, 1]$ is the rate parameter (probability of heads). If the data are iid, the likelihood has the form \n",
    "\n",
    "$$\n",
    "p(\\mathcal{D}|\\theta) = \\theta^{N_1}(1-\\theta)^{N_0}\n",
    "$$\n",
    "\n",
    "where we have $N_1=\\sum_{i=1}^N\\mathbb{I}(x_i=1)$ heads and $N_0 = \\sum_{i=1}^N\\mathbb{I}(x_i=0)$ tails. These two counts are called the **sufficient statistics** of the data, since this is all we need to know about $\\mathcal{D}$ to infer $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior\n",
    "We need a prior which has support over the interval [0, 1]. To make the math easier, it would be convenient if the prior had the same form as the likelihood, i.e. if the prior looked like\n",
    "\n",
    "$$\n",
    "p(\\theta)\\propto\\theta^{\\gamma_1}(1-\\theta)^{\\gamma_2}\n",
    "$$\n",
    "\n",
    "For some prior parameters $\\gamma_1$ and $\\gamma_2$, in which case we can evaluate the posterior by simply adding up the exponents. When the prior and posterior have the same form, we say that the prior is a **conjugate prior** for the corresponding likelihood. \n",
    "\n",
    "In the case of the Bernoulli, the conjugate prior is the beta distribution:\n",
    "\n",
    "$$\n",
    "\\mathrm{Beta}(\\theta|a, b) \\propto \\theta^{a-1}(1-\\theta)^{b-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior\n",
    "If we multiply the likelihood by the beta prior we get the following posterior:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "p(\\theta|\\mathcal{D}) &\\propto & p(\\mathcal{D}|\\theta)p(\\theta) \\propto \\theta^{N_1}(1-\\theta)^{N_0}\\theta^{a-1}(1-\\theta)^{b-1} \\\\\n",
    "   & \\propto & \\theta^{N_1+a-1}(1-\\theta)^{N_0+b-1}\\\\\n",
    "   & \\propto & \\mathrm{Beta}(N_1+a, N_0+b)\n",
    "\\end{eqnarray}\n",
    "\n",
    "In particular, the posterior is obtained by adding the prior hyperparameters to the empirical counts. \n",
    "\n",
    "#### Posterior mean and mode\n",
    "The MAP estimate is given by \n",
    "$$\n",
    "\\hat{\\theta}_{MAP} = \\frac{a + N_1 -1}{a + b + N - 2}\n",
    "$$\n",
    "(from the definition of the Beta distribution). If we use a uniform prior (i.e. $a = 1$, $b = 1$), then the MAP estimate reduces to the MLE which is just the empirical fraction of heads:\n",
    "$$\n",
    "\\hat{\\theta}_{MLE} = \\frac{N_1}{N}\n",
    "$$\n",
    "\n",
    "**Exercise 3.1**: We can derive this by maximising the likelihood function $p(\\mathcal{D}|\\theta) = \\theta^{N_1}(1-\\theta)^{N_0}$. Taking the derivative of the $\\log$ of the likelihood we obtain:\n",
    "$$\n",
    "\\frac{d\\log p}{d\\theta} = \\frac{N_1}{\\theta} - \\frac{N_0}{1-\\theta}\n",
    "$$\n",
    "To find the maximum set the derivative to zero, where we find that \n",
    "$$\n",
    "\\hat{\\theta}_{MLE} = \\frac{N_1}{N_0+N_1} = \\frac{N_1}{N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior mean is given by\n",
    "$$\n",
    "\\bar{\\theta} = \\frac{a + N_1}{a + b + N}\n",
    "$$\n",
    "\n",
    "We show that the posterior mean is a convex combination of the prior mean and the MLE, which captures the notion that the posterior is a compromise between what we previously believed and what the data is telling us.\n",
    "\n",
    "Let $\\alpha_0 = a + b$ be the equivalent sample size of the prior, which controls its strength, and let the prior mean be $m_1 = a/\\alpha_0$. Then the posterior mean is given by\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\theta|\\mathcal{D}] = \\frac{\\alpha_0m_1 + N_1}{N+ \\alpha_0} = \\frac{\\alpha_0}{N+\\alpha_0}m_1 + \\frac{N}{N+\\alpha_0}\\frac{N_1}{N} = \\lambda m_1 + (1-\\lambda)\\hat{\\theta}_{MLE}\n",
    "$$\n",
    "\n",
    "where $\\lambda=\\frac{\\alpha_0}{N + \\alpha_0}$ is the ratio of the prior to posterior equivalent sample size. So the weaker the prior, the smaller is $\\lambda$, and hence the closer the posterior mean is to the MLE as $N\\rightarrow\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posterior Variance\n",
    "The variance of the Beta posterior is given by\n",
    "$$\n",
    "\\mathrm{var}[\\theta|\\mathcal{D}] = \\frac{(a+N_1)(b+N_2)}{(a+N_1+b+N_0)^2(a+N_1+b+N_0+1)}\n",
    "$$\n",
    "Simplify this in the case that $N \\gg a, b$, to get\n",
    "$$\n",
    "\\mathrm{var}[\\theta|\\mathcal{D}]\\approx\\frac{N_1N_0}{NNN}=\\frac{\\hat{\\theta}(1-\\hat{\\theta})}{N}\n",
    "$$\n",
    "where $\\hat{\\theta}$ is the MLE. Hence the error bar (posterior standard deviation) is given by\n",
    "$$\n",
    "\\sigma = \\sqrt{\\mathrm{var}[\\theta|\\mathcal{D}]}\\approx\\sqrt{\\frac{\\hat{\\theta}(1-\\hat{\\theta})}{N}}\n",
    "$$\n",
    "So uncertainty goes down at a rate of $1/\\sqrt{N}$. Note however the uncertainty is maximized when $\\hat{\\theta} = 0.5$, which means it is easier to be sure that a coin is biased than to be sure that it is fair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the outcome of future trials\n",
    "The posterior predictive is given by the following, known as the compound **beta-binomial** distribution\n",
    "$$\n",
    "Bb(x|a, b, M) \\triangleq \\binom{M}{x}\\frac{B(x+a, M-x+b)}{B(a, b)}\n",
    "$$\n",
    "This distribution has the following mean and variance\n",
    "$$\n",
    "\\mathbb{E}[x] = M\\frac{a}{a+b}, \\mathrm{var}[x] = \\frac{Mab}{(a+b)^2}\\frac{(a+b+M)}{a + b + 1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
