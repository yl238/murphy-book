{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphical Models\n",
    "These are my notes from chapter 10 of Murphy, they will be tidied up and written into a blog at some point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The chain rule\n",
    "Multiplying out the conditional probability\n",
    "$$\n",
    "P(X_1, X_2, \\ldots, X_n) = P(X_1|X_2,\\ldots,X_n)P(X_2|X_3,\\ldots,X_n)\\cdots P(X_{n-1}|X_n)P(X_n)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Independence\n",
    "The key to efficiently representing large joint distributions is to make some assumptions about conditional independence (CI). Two random variables $X$ and $Y$ are conditionally independent given $Z$, denoted $X \\perp Y | Z$, iff the conditional joint distribution can be written as a product of conditional marginals, i.e.\n",
    "\n",
    "$$\n",
    "X\\perp Y | Z \\Longleftrightarrow p(X, Y |Z) = p(X|Z)p(Y|Z)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphical Models\n",
    "A **graphical model (GM)** is a way to represent a joint distribution by making CI assumptions. In particular, the nodes in the graph represent random variables, and the (lack of) edges represent CI assumptions. \n",
    "\n",
    "A **directed graphical model** or **DGM** is a GM whose graph is a **directed acyclic graph** or DAG. This is a directed graph with no directed cycles. These are commonly known as **Bayesian networks**, or **belief networks**.\n",
    "\n",
    "Formally, a belief network is a distribution of the form:\n",
    "$$\n",
    "P(x_1,\\ldots,x_D) = \\prod_{i=1}^D P(x_i|\\mathbf{Pa}(x_i))\n",
    "$$\n",
    "\n",
    "The key property of DAGs is that the nodes can be ordered such that parents come before children. This is called a topological ordering, and it can be constructed from any DAG. Given such an order, we define the **ordered Markov property** to be the assumption that a node only depends on its immediate parents, not on all predecessors in the ordering, i.e.\n",
    "\n",
    "$$\n",
    "x_s \\perp \\mathbf{x}_{\\mathrm{pred}(s)\\backslash \\mathrm{pa}(s)}|\\mathbf{x}_{\\mathrm{pa}(s)}\n",
    "$$\n",
    "\n",
    "where $\\mathrm{pa}(s)$ are the parents of the node $s$, and $\\mathrm{pred}(s)$ are the predecessors of node $s$ in the ordering. This is a natural generalisation of the first-order Markov property to from chains to general DAGs.\n",
    "\n",
    "In general, the joint probability distribution of a DGM is given by\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}_{1:V}|G) = \\prod_{t=1}^V p(x_t|\\mathbf{x}_{\\mathrm{pa}(t)})\n",
    "$$\n",
    "\n",
    "where each term $p(x_t|\\mathbf{x}_{\\mathrm{pa}(t)})$ is a **conditional probability distribution** (CPD). we have written the distribution as $p(\\mathbf{x}|G)$ to emphasise that this equation only holds if the CI assumptions encoded in DAG $G$ are correct. However, we will usually drop this explicit conditioning for brevity. If each node has $O(F)$ parents and $K$ states, the number of parameters in the model is $O(VK^F)$, which is much less than the $O(K^V)$ needed by a model which makes no CI assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factors\n",
    "The concept of *factors* is important to PGMS.\n",
    "\n",
    "A **factor** is a function $\\phi(X_1,\\ldots,X_k)$ which takes all possible combinations of outcomes (assignments) for these random variables $X_1,\\ldots,X_k$ and gives a real value for each combination.\n",
    "\n",
    "The set of random variables $\\{X_1,\\ldots,X_k\\}$ is called the *scope* of the factor.\n",
    "\n",
    "A joint distribution is a factor which returns a number which is the probability of a given combination of assignments. An unnormalized measure is also a factor, e.g. $P(I, D, g^1)$.\n",
    "\n",
    "A conditional probability distribution (CPD) is also a factor, e.g. $P(G|I, D)$. \n",
    "\n",
    "A common operation on factors is a *factor product*. Say we have the factors $\\phi_1(A, B)$ and $\\phi_2(B, C)$. Their factor product would yield a new factor $\\phi_3(A, B, C)$. The result for a given combo $a_i, b_j, c_k$ is just $\\phi_1(a_i, b_j)\\cdot\\phi(b_j, c_k)$.\n",
    "\n",
    "Another operation is *factor marginalization*. This is the same as marginalization for probability distributions but generalized for all factors. For example, $\\phi(A, B, C)\\rightarrow\\phi(A, B)$. \n",
    "\n",
    "Another operation is *factor reduction* which is similarly a generalization of probability distribution reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
