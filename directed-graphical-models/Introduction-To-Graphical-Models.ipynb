{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphical Models\n",
    "These are my notes from chapter 10 of Murphy, they will be tidied up and written into a blog at some point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Independence\n",
    "The key to efficiently representing large joint distributions is to make some assumptions about conditional independence (CI). Two random variables $X$ and $Y$ are conditionally independent given $Z$, denoted $X \\perp Y | Z$, iff the conditional joint distribution can be written as a product of conditional marginals, i.e.\n",
    "\n",
    "$$\n",
    "X\\perp Y | Z \\Longleftrightarrow p(X, Y |Z) = p(X|Z)p(Y|Z)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphical Models\n",
    "A **graphical model (GM)** is a way to represent a joint distribution by making CI assumptions. In particular, the nodes in the graph represent random variables, and the (lack of) edges represent CI assumptions. \n",
    "\n",
    "A **directed graphical model** or **DGM** is a GM whose graph is a **directed acyclic graph** or DAG. This is a directed graph with no directed cycles. These are commonly known as **Bayesian networks**, or **belief networks**.\n",
    "\n",
    "The key property of DAGs is that the nodes can be ordered such that parents come before children. This is called a topological ordering, and it can be constructed from any DAG. Given such an order, we define the **ordered Markov property** to be the assumption that a node only depends on its immediate parents, not on all predecessors in the ordering, i.e.\n",
    "\n",
    "$$\n",
    "x_s \\perp \\mathbf{x}_{\\mathrm{pred}(s)\\backslash \\mathrm{pa}(s)}|\\mathbf{x}_{\\mathrm{pa}(s)}\n",
    "$$\n",
    "\n",
    "where $\\mathrm{pa}(s)$ are the parents of the node $s$, and $\\mathrm{pred}(s)$ are the predecessors of node $s$ in the ordering. This is a natural generalisation of the first-order Markov property to from chains to general DAGs.\n",
    "\n",
    "In general, the joint probability distribution of a DGM is given by\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}_{1:V}|G) = \\prod_{t=1}^V p(x_t|\\mathbf{x}_{\\mathrm{pa}(t)})\n",
    "$$\n",
    "\n",
    "where each term $p(x_t|\\mathbf{x}_{\\mathrm{pa}(t)})$ is a **conditional probability distribution** (CPD). we have written the distribution as $p(\\mathbf{x}|G)$ to emphasise that this equation only holds if the CI assumptions encoded in DAG $G$ are correct. However, we will usually drop this explicit conditioning for brevity. If each node has $O(F)$ parents and $K$ states, the number of parameters in the model is $O(VK^F)$, which is much less than the $O(K^V)$ needed by a model which makes no CI assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
