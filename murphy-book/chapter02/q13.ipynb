{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.13 Mutual information for correlated normals\n",
    "(Source: (Cover and Thomas 1991, Q9.3).) Find the mutual information $I(X_1,X_2)$ where $X$ has a bivariate normal distribution:\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}XX_1 \\\\ X_2\\end{array}\\right)\\sim\\mathcal{N}\\left(\\mathbf{0},\\left(\\begin{array}{ll}\\sigma^2 & \\rho\\sigma^2\\\\\n",
    "\\rho\\sigma^2 & \\sigma^2 \\end{array}\\right)\\right)\n",
    "$$\n",
    "\n",
    "Evaluate $I(X_1, X_2)$ for $\\rho = 1$, $\\rho = 0$ and $\\rho = âˆ’1$ and comment. Hint: The (differential) entropy of a $d$-dimensional Gaussian is \n",
    "\n",
    "$$\n",
    "h(X) = \\frac{1}{2}\\log_2\\left[(2\\pi e)^d\\mathrm{det}\\,\\Sigma\\right]\n",
    "$$ \n",
    "\n",
    "In the 1d case, this becomes\n",
    "\n",
    "$$\n",
    "h(X) = \\frac{1}{2}\\log_2[2\\pi e\\sigma^2]\n",
    "$$\n",
    "\n",
    "Hint: $\\log(\\theta) =\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Info\n",
    "\n",
    "In this question, we have to calculate the mutual information of two correlated Gaussian random variables. We should expect large mutual information values, when there is a linear relationship between the random variables (i.e. $\\rho\\pm 1$). On the other hand, we should expect small mutual information values, when the variables are decorrelated ($\\rho = 0$).\n",
    "\n",
    "NOTE: This question assumes we know how to calculate the distribution $p(x_1|x_2)$ where $X_1$ and $X_2$ are random variables with a bivariate normal distribution. Furthermore, this is not a straightforward result so we need to take it on authority that it is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "Let $x = (x_1, x_2)$ have a joint Gaussian distribution with parameters:\n",
    "\n",
    "$$\n",
    "\\mu = \\left[\\begin{array}{l} \n",
    "\\mu_1 \\\\\\mu_2\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Sigma = \\left[\\begin{array}{l}\\Sigma_{11} & \\Sigma_{12} \\\\\n",
    "\\Sigma_{21} & \\Sigma_{22} \\end{array}\\right]\n",
    "$$\n",
    "\n",
    "Then the conditional probability $p(x_1|x_2)$ is also Gaussian and its parameters are:\n",
    "\n",
    "\\begin{aligned}\n",
    "p(x_1| x_2) & = \\mathcal{N}(x_1|\\mu_{1|2}, \\Sigma_{1|2}) \\\\\n",
    "\\mu_{1|2}  & = \\mu_1 + \\Sigma_{12}\\Sigma^{-1}_{22}(x_2 - \\mu_2) \\\\\n",
    "\\Sigma_{1|2} & = \\Sigma_{11}-\\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}\n",
    "\\end{aligned}\n",
    "\n",
    "Hold this result for now and go to the actual question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mutual information is given by\n",
    "\n",
    "$$\n",
    "I(X_1, X_2) = H(X_1) - H(X_1|X_2)\n",
    "$$\n",
    "\n",
    "So we need to calculate the two entropies. The first one is straightforward, because $X\\sim\\mathcal{N}(0,\\sigma^2)$:\n",
    "\n",
    "$$\n",
    "H(X) = \\frac{1}{2}\\log_2[2\\pi\\,e\\sigma^2]\n",
    "$$\n",
    "\n",
    "The other is also straightforward, since we know that $X_1|X_2$ is Gaussian and we know how to calculate the standard deviation. To calculate the standard deviation, we must first determine $\\Sigma$ for this distribution.\n",
    "\n",
    "$$\n",
    "\\Sigma = \\left[\\begin{array}{ll} \\Sigma_{11} & \\Sigma_{12}\\\\ \\Sigma_{21} & \\Sigma_{22}\\end{array}\\right] = \\left[\\begin{array}{ll}\\sigma^2 & \\rho\\sigma^2\\\\\n",
    "\\rho\\sigma^2 & \\sigma^2 \\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now arrive at:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\Sigma_{1|2} = \\sigma^2_{1|2} = \\sigma^2-\\rho\\sigma^2\\frac{1}{\\sigma^2}\\rho\\sigma^2 & = \\sigma^2(1-\\rho^2)\\\\\n",
    "\\sigma_{1|2} = \\sigma\\sqrt{(1-\\rho^2)}\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing the standard deviation of $X_1|X_2$, we can calculate the conditional entropy:\n",
    "\n",
    "$$\n",
    "H(X_1|X_2) = \\frac{1}{2}\\log_2\\left[2\\pi e\\sigma^2(1-\\rho^2)\\right]\n",
    "$$\n",
    "\n",
    "Substituting into the equation for mutual information, we get:\n",
    "\n",
    "$$\n",
    "I(X_1, X_2) = H(X_1) - H(X_1|X_2) = \\frac{1}{2}\\left[\\frac{1}{(1-\\rho^2)}\\right]\n",
    "$$\n",
    "\n",
    "For $\\rho=\\pm 1$,\n",
    "\n",
    "$$\n",
    "I(X_1, X_2) = \\frac{1}{2}\\log_2[1/(1-1)] = \\infty\n",
    "$$\n",
    "\n",
    "for $\\rho = 0$,\n",
    "\n",
    "$$\n",
    "I(X_1, X_2 ) = \\frac{1}{2}\\log_2|1 / (1-0)| = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "As we said in the introduction, the mutual information goes to larger value (infinity) when the two random variables are linearly related, and it goes to zero when there is no correlation between the variables.\n",
    "\n",
    "It is worthy to note one key element of this result. The mutual information was introduced with the argument of being a more general measure of dependence between two variables than the correlation coefficient. However, in this scenario, the correlation coefficient is as powerful as the mutual information. Afterall, $\\rho=0$ iff $I(X, Y) = 0$. Therefore, if we have a bivariate normal distribution and $\\rho=0$, then $X_1$ and $X_2$ are independent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
