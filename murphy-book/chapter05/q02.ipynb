{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.2 Optimal threshold on classification probability\n",
    "Consider a case where we have learned a conditional probability distribution $P(y|\\mathbf{x})$. Suppose there are only two classes, and let $p_0 = P(Y = 0|\\mathbf{x})$ and $p_1 = P (Y = 1|\\mathbf{x}$). Consider the loss matrix below:\n",
    "\n",
    "|Predicted label $\\hat{y}$ | True | label y |\n",
    "|:---|:---|:---|\n",
    "|   | 0 | 1|\n",
    "|0 | 0|$\\lambda_{01}$|\n",
    "|1 |$\\lambda_{10}$ | 0|\n",
    "\n",
    "a. Show that the decision $\\hat{y}$ that minimizes the expected loss is equivalent to setting a probability threshold $\\theta$ and predicting $\\hat{y}=0$ if $p_1 < θ$ and $\\hat{y}=1$ if $p_1 ≥θ$. What is θ as a function of $λ_{01}$ and $λ_{10}$? (Show your work.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "In Bayesian decision theory, the optimal action, having observed $\\mathbf{x}$ is defined as the action $a$ that minimises the **posterior expected loss**:\n",
    "\n",
    "$$\n",
    "\\rho(a|\\mathbf{x})\\triangleq\\mathbb{E}_{p(y|x)}[L(y, a)] = \\sum_yL(y, a)p(y|\\mathbf{x})\n",
    "$$\n",
    "\n",
    "Hence, the **Bayes estimator**, or **Bayes decision rule** is given by \n",
    "\n",
    "$$\n",
    "\\delta(\\mathbf{x}) = \\mathrm{arg}\\min_{a\\in\\mathcal{A}}\\rho(\\mathbf{a}|\\mathbf{x})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expression of our expected loss is: \n",
    "\n",
    "$$\n",
    "\\rho(a|\\mathbf{x}) = p_0L(0, a) + p_1L(1, a) = p_0L(0,\\hat{y}) + p_1L(1,\\hat{y}) = p_0\\lambda_{10}\\hat{y} + p_1\\lambda_{01}(1-\\hat{y})\n",
    "$$\n",
    "\n",
    "If $\\hat{y} = 0$, $\\rho(a|\\mathbf{x}) = p_1\\lambda_{01}$, and if $\\hat{y} = 1$, $\\rho(a|\\mathbf{x}) = p_0\\lambda_{10}$,\n",
    "\n",
    "Therefore, if $p_1\\lambda_{01} > p_0\\lambda_{10} = (1-p_1)\\lambda_{10}$, we have to take $\\hat{y} = 1$. Otherwise, $\\hat{y} = 0$.\n",
    "\n",
    "The condition is equivalent to $p_1 > \\theta$ where \n",
    "\n",
    "$$\n",
    "\\theta = \\frac{\\lambda_{10}}{\\lambda_{01} + \\lambda_{10}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Show a loss matrix where the threshold is 0.1. (Show your work.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "0.1 = \\frac{\\lambda_{10}}{\\lambda_{01}+\\lambda_{10}} \\implies \\lambda_{01} = 9\\lambda_{10}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes since since $\\theta$ should be small when the misclassification error $\\lambda_{10}$ is much smaller than $\\lambda_{01}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
