{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1 MLE for the Bernoulli/ binomial model\n",
    "Derive Equation 3.22 by optimizing the log of the likelihood in Equation 3.11."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, we will derived the maximum likelihood estimator for the Bernoulli / binomial model. Let's remind ourselves of what these models are:\n",
    "1. The Bernoulli model says that when we have a dataset $\\mathcal{D}$ formed by the result of $N$ coin tosses, with $N_1$ heads and $N_0$ tails, the likelihood of that event is \n",
    "\n",
    "$$\n",
    "p_{Ber}(\\mathcal{D}|\\theta) = \\theta^{N_1}(1-\\theta)^{N_0}\n",
    "$$\n",
    "\n",
    "2. The Binomial model says that the probability of getting $N_1$ heads in $N$ trials is given by \n",
    "$$\n",
    "p_{Bin}(\\mathcal{D}|\\theta) = \\left(\\begin{array}{l}N\\\\N_1\\end{array}\\right)\\theta^{N_1}(1-\\theta)^{N-N_1}\n",
    "$$\n",
    "The binomial coefficient $\\left(\\begin{array}{l}N\\\\N_1\\end{array}\\right)$ was introduced to take into account the lack of order in the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bernoulli model gives the probability of a specific sequence of events, while the Binomial model gives probability of all sequence of events that satisfy a given property (in this case, the number of heads). \n",
    "\n",
    "Since their expressions are proportional, maximising one is the same as maximising the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "The likelihood of the Bernoulli model is given by:\n",
    "\n",
    "$$\n",
    "p(\\mathcal{D}|\\theta) = \\theta^{N_1}(1-\\theta)^{N_0}\n",
    "$$\n",
    "\n",
    "To find the argument that maximises this function $p$, it is sufficient for us to find the argument that maximises any increasing monotonic function $f(p)$. Using the log of the likelihood,\n",
    "\n",
    "$$\n",
    "\\log(p) = N_1\\log(\\theta) + N_0\\log(1-\\theta)\n",
    "$$\n",
    "\n",
    "Now we just have to differentiate it and set it equal to zero:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\frac{d\\log(p)}{d\\theta} & = 0 \\\\\n",
    "N_1\\frac{1}{\\theta} - N_0\\frac{1}{1-\\theta} & = 0\n",
    "\\end{aligned}\n",
    "\n",
    "Therefore, \n",
    "\n",
    "$$\n",
    "\\theta_{\\mathrm{MLE}} = \\frac{N_1}{N_0+N_1} = \\frac{N_1}{N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum likelihood estimator is just the empirical count of heads $N_1/N$. The dangers of MLE is that the MLE estimator can suffer from the **zero count problem**.\n",
    "\n",
    "For example, if we toss a coin 100 times and they all come up tails, then $\\theta_{\\mathrm{MLE}} = 0$, which says there is no chance of getting heads on this coin, which is ridiculous. We solve this by being Bayesian and using priors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
