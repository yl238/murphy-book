{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.21 Mutual information for naive Bayes classifiers with binary features \n",
    "Derive Equation 3.76."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mutual information between feature $X_j$ and the class label $Y$ is given by \n",
    "\n",
    "$$\n",
    "I(X, Y) = \\sum_{x_j}\\sum_{y}p(x_j, y)\\log\\frac{p(x_j, y)}{p(x_j)p(y)}\n",
    "$$\n",
    "\n",
    "The mutual information can be thought of as the reduction in entropy on the label distribution once we observe the value of feature $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the features are binary, then\n",
    "\n",
    "\\begin{aligned}\n",
    "I_j & = \\sum_y p(x_j=0, y)\\log\\frac{p(x_j=0, y)}{p(x_j=0)p(y)} + p(x_j=1, y)\\log\\frac{p(x_j=1, y)}{p(x_j=1)p(y)} \\\\\n",
    "& = \\sum_y p(x_j=0|y)p(y)\\log\\frac{p(x_j=0| y)}{p(x_j=0)} + p(x_j=1|y)p(y)\\log\\frac{p(x_j=1|y)}{p(x_j=1)} \\\\\n",
    "& = \\sum_{c}(1-p(x_j=1|y=c))p(y=c)\\log\\frac{(1-p(x_j=1|y=c))}{(1-p(x_j=1))} + p(x_j=1|y=c)p(y=c)\\log\\frac{p(x_j=1|y=c)}{p(x_j=1)}\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we set $\\pi_c = p(y=c)$, $\\theta_{jc} = p(x_j=1|y=c)$, and $\\theta_j = p(x_j=1) = \\sum_c\\pi_c\\theta_{jc}$, then we have\n",
    "\n",
    "$$\n",
    "I_j = \\sum_c\\left[(1-\\theta_{jc})\\pi_c\\log\\frac{1-\\theta_{jc}}{1-\\theta_j}+ \\theta_{jc}\\pi_c\\log\\frac{\\theta_{jc}}{\\theta_j}\\right]\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
