{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifiers\n",
    "\n",
    "We want to classify vectors of discrete value features, $\\mathbf{x}\\in\\{1,\\ldots,K\\}^D$, where $K$ is the number of values for each feature, and $D$ is the number of features. If we use a generative approach, we will need to specify the class conditional distribution $p(\\mathbf{x}|y=c)$.\n",
    "\n",
    "The simplest approach is to assume that the features are **conditionally independent** given the class label. This means we can write the class conditional density as a product of one-dimensional densities:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}|y=c, \\theta) = \\prod_{j=1}^Dp(x_j|y=c,\\theta_{jc})\n",
    "$$\n",
    "\n",
    "The resulting model is called a Naive Bayes Classifier (NBC). This has $O(CD)$ parameters, for $C$ classes and $D$ features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting\n",
    "'Training' a Naive Bayes classifier usually means computing the MLE or MAP estimate for the parameters. We can also compute the full posterior $p(\\theta|\\mathcal{D})$\n",
    "\n",
    "### MLE\n",
    "the probability for a single data case is given by\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}_i, y_i|\\theta) = p(y_i|\\pi)\\prod_jp(x_{ij}|\\theta_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
