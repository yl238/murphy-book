{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifiers\n",
    "\n",
    "We want to classify vectors of discrete value features, $\\mathbf{x}\\in\\{1,\\ldots,K\\}^D$, where $K$ is the number of values for each feature, and $D$ is the number of features. If we use a generative approach, we will need to specify the class conditional distribution $p(\\mathbf{x}|y=c), c\\in\\{1, 2, \\ldots, K\\}$.\n",
    "\n",
    "The simplest approach is to assume that the features are **conditionally independent** given the class label. This means we can write the class conditional density as a product of one-dimensional densities:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}|y=c, \\theta) = \\prod_{j=1}^Dp(x_j|y=c,\\theta_{jc})\n",
    "$$\n",
    "\n",
    "The resulting model is called a Naive Bayes Classifier (NBC). This has $O(CD)$ parameters, for $C$ classes and $D$ features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting\n",
    "'Training' a Naive Bayes classifier usually means computing the MLE or MAP estimate for the parameters. We can also compute the full posterior $p(\\theta|\\mathcal{D})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume throughout that each vector $\\mathbf{x}$ is in the set $\\{0, 1\\}^D$ for the integer $D$, which is the number of features. In other words, each component $x_j$ for $j=1,\\ldots,D$ can take one of two possible values.\n",
    "\n",
    "### Example: classifying documents into $K$ different categories\n",
    "For example, $y=1$ might corresponds to a *sports* category, $y=2$ might correspond to a *politics* category, and so on.\n",
    "\n",
    "The label $y^{(i)}$ represents the category of the $i$-th document in the dataset. Each component $x_j^{(i)}$ for $j=1,\\ldots,D$ might represent the presence or absence of a particular word. For example, we might define $x_1^{(i)}$ to be 1 if the $i$-th document contains the word *Giants*, or zero otherwise; $x_2^{(i)}$ to be 1 if the $i$-th document contains the word *Obama* and zero otherwise, and so on.\n",
    "\n",
    "The Naive Bayes model is derived as follows: we assume random variables $Y$ and $\\mathbf{x}_1,\\ldots,\\mathbf{x}_D$ corresponding to the label $y$ and vector components $x_1,x_2,\\ldots,x_D$. Our task is to model the joint probability\n",
    "\n",
    "$$\n",
    "p(y=c, \\mathbf{x}_1=x_1, \\mathbf{x}_2=x_2, \\mathbf{x}_D = x_D)\n",
    "$$\n",
    "\n",
    "for any label $y$ paired with attribute values $x_1,\\ldots,x_D$. A key idea in the Naive Bayes model is the following assumptions\n",
    "\n",
    "\\begin{aligned}\n",
    "p(y=c, \\mathbf{x}_1=x_1, \\mathbf{x}_2=x_2, \\mathbf{x}_D = x_D) = p(y=c)\\prod_{j=1}^D p(\\mathbf{x}_j=x_j|y=c)\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we can write\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}|y = c) = \\prod_{j=1}^Dp(\\mathbf{x}_j=x_j|y=c)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following this equation, the NB model has two types of parameters: $\\pi(y)$ for $y \\in\\{1,\\ldots,K\\}$, with \n",
    "\n",
    "$$\n",
    "p(y=c) = \\pi(y)\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$\\theta_j(x|y)$ for $j\\in\\{1,\\ldots,D\\}$, $x\\in\\{0, 1\\}$, $y\\in\\{1,\\ldots,K\\}$, with\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}_j = x_j|y=c) = \\theta_j(x|y)\n",
    "$$\n",
    "\n",
    "We then have\n",
    "\n",
    "$$\n",
    "p(y, x_1,\\ldots, x_D) = \\pi(y)\\prod_{j=1}^D \\theta_j(x_j|y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition 1: Naive Bayes Model\n",
    "A Naive Bayes model consists of an inteter $K$ specifying the number of possible labels, an integer $D$ specifying the number of features, and in addition the following parameters:\n",
    "* $\\pi(y)$ for any $y\\in\\{1,\\ldots,K\\}$. The parameter $\\pi(y)$ can be interpreted as the probability of seeing the label $y$. We have the constraints $\\pi(y)\\ge 0$ and $\\sum_{y=1}^K\\pi(y)=1$.\n",
    "* A parameter $\\theta_j(x|y)$ for any  $j\\in\\{1,\\ldots,D\\}, x\\in\\{0, 1\\}, y\\in\\{1,\\ldots,K\\}$. \n",
    "\n",
    "The value for $\\theta_j(x|y)$ can be interpreted as the probability of feature $j$ taking the value $x$, conditioned on the underlying label being $y$. We have the constraints that $\\theta_j(x|y)\\ge 0$, and for all $y, j$, $\\sum_{x\\in\\{0, 1\\}}\\theta_j(x|y) = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the parameters have been estimated, given a new test example $\\hat{\\mathbf{x}} = (x_1,x_2,\\ldots,x_D)$, the output of the NB classifier is\n",
    "\n",
    "$$\n",
    "\\mathrm{argmax}_{y\\in\\{1,\\ldots,K\\}}p(y,x_1,\\ldots,x_D) = \\mathrm{argmax}_{y\\in\\{1,\\ldots,K\\}}\\left(\\pi(y)\\prod_{j=1}^D\\theta_j(x_j|y)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE\n",
    "the probability for a single data case is given by\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}_i, y_i|\\theta) = p(y_i|\\pi)\\prod_jp(x_{ij}|\\theta_j) = \\prod_c\\pi_c^{\\mathbb{I}(y_i=c)}\\prod_j\\prod_c p(x_{ij}|\\boldsymbol\\theta_{jc})^{\\mathbb{I}(y_i=c)}\n",
    "$$\n",
    "\n",
    "Hence the log-likelihood is given by\n",
    "\n",
    "$$\n",
    "\\log p(\\mathcal{D}|\\boldsymbol\\theta) = \\sum_{c=1}^C N_c\\log\\pi_c + \\sum_{j=1}^D\\sum_{c=1}^C\\sum_{i:y_i=c}\\log p(x_{ij}|\\boldsymbol\\theta_{jc})\n",
    "$$\n",
    "\n",
    "We see that this expression decomposes into a series of terms, one concerning $\\boldsymbol\\pi$, and $DC$ terms containing the $\\boldsymbol\\theta_{jc}$s. Hence we can optimize all parameters separately.\n",
    "\n",
    "The MLE for the class prior is given by \n",
    "\n",
    "$$\n",
    "\\hat{\\pi}_c = \\frac{N_c}{N}\n",
    "$$\n",
    "\n",
    "where $N_c\\triangleq \\sum_i\\mathbb{I}(y_i=c)$ is the number of examples in class $c$.\n",
    "\n",
    "The MLE for the likelihood depends on the type of distribution we choose to use for each feature. For simplicity, let us suppose all features are binary, so $x_j|y = c\\sim \\mathrm{Ber}(\\theta_{jc})$. In this case, the MLE becomes\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_{jc} = \\frac{N_{jc}}{N_c}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Naive Bayes\n",
    "The trouble with maximum likelihood is that it can overfit. A simple solution to overfitting is to be Bayesian. For simplicity, we will use a factored prior:\n",
    "\n",
    "$$\n",
    "p(\\boldsymbol\\theta) = p(\\boldsymbol\\pi)\\prod_{j=1}^D\\prod_{c=1}^Cp(\\theta_{jc})\n",
    "$$\n",
    "\n",
    "We will use a $\\mathrm{Dir}(\\boldsymbol\\alpha)$ prior for $\\boldsymbol\\pi$ and a $\\mathrm{Beta}(\\beta_0, \\beta_1)$ prior for each $\\theta_{jc}$. Often we just take $\\boldsymbol\\alpha=1$ and $\\boldsymbol\\beta=1$ corresponding to add-one or Laplace smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the factored likelihood with the factored prior above, gives the following factored posterior\n",
    "\n",
    "\\begin{aligned}\n",
    "p(\\boldsymbol\\theta|\\mathcal{D} & = p(\\boldsymbol\\pi|\\mathcal{D})\\prod_{j=1}^D\\prod_{c=1}^C p(\\theta_{jc}|\\mathcal{D}) \\\\\n",
    "p(\\pi|\\mathcal{D}) & = \\mathrm{Dir}(N_1 + \\alpha_1, \\ldots, N_c+\\alpha_c) \\\\\n",
    "p(\\theta_{jc}|\\mathcal{D}) & = \\mathrm{Beta}((N_c - N_{jc}) + \\beta_0, N_jc +\\beta_1)\n",
    "\\end{aligned}\n",
    "\n",
    "In order words, to compute the posterior, we just update the prior counts with empirical counts from the likelihood"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
