{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.11 Bayesian analysis of the exponential distribution\n",
    "A lifetime $X$ of a machine is modeled by an exponential distribution with unknown parameter θ. The likelihood is $p(x|θ) = θe^{−θx}$ for $x ≥ 0$, $θ > 0$.\n",
    "\n",
    "(a) Show that the MLE is $\\hat{θ}= 1/\\hat{x}$, where $\\hat{x} = \\frac{1}{N}\\sum^N_{i=1}x_i$.\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "The log-likelihood $l(\\theta)$ is given by\n",
    "\n",
    "$$\n",
    "l(\\theta) = \\sum_{i=1}^N\\log(\\theta e^{-\\theta x_i}) \\propto \\sum_{i=1}^N\\log\\theta-\\theta x_i) = N\\log\\theta - \\theta\\sum_{i=1}^N x_i\n",
    "$$\n",
    "\n",
    "To obtain the MLE, we differentiate the above equation\n",
    "\n",
    "\\begin{aligned}\n",
    "\\frac{dl(\\theta)}{d\\theta} & = 0 \\\\\n",
    "N\\frac{1}{\\theta} - \\sum_{i=1}^N x_i & = 0\\\\\n",
    "\\theta_{\\mathrm{MLE}} & = \\frac{N}{\\sum_{i=1}^N x_i} = \\frac{1}{\\bar{x}}\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Suppose we observe $X_1$ = 5, $X_2$ = 6, $X_3 = 4$ (the lifetimes (in years) of 3 different iid machines).\n",
    "What is the MLE given this data?\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "MLE = 3/(5+6+4) = 1/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Assume that an expert believes θ should have a prior distribution that is also exponential\n",
    "\n",
    "$$\n",
    "p(θ) = \\mathrm{Expon}(θ|λ)\n",
    "$$\n",
    "\n",
    "Choose the prior parameter, call it $\\hat{λ}$, such that $\\mathbb{E}[θ] = 1/3$. Hint: recall that the Gamma distribution has the form\n",
    "$\\mathbf{Ga}(θ|a, b) \\propto θ^{a−1}e^{−θ}b$ and its mean is $a/b$.\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "The exponential distribution $\\mathrm{Expon}(\\theta|\\lambda)$ is a particular case of the Gamma distribution: $\\mathrm{Expon}(\\theta|\\lambda) = \\mathrm{Ga}(\\theta|1,\\lambda)$. Therefore, we can use the mean of the Gamma distribution to calculate $\\hat{\\lambda}$:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}[\\theta]_{\\mathrm{Expon}(\\theta|\\lambda)} & = \\mathbb{E}[\\theta]_{\\mathrm{Ga}(\\theta|1,\\lambda)} = \\frac{1}{\\lambda} = \\frac{1}{3} \\\\\n",
    "\\hat{\\lambda} & = 3\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) What is the posterior, $p(θ|D, λ)$?\n",
    "\n",
    "**Solution**\n",
    "\n",
    "In order to calculate the posterior, we need to calculate the prior and the likelihood.\n",
    "\n",
    "Prior:\n",
    "\n",
    "$$\n",
    "p(\\theta|\\hat{\\lambda}) = \\mathrm{Expon}(\\theta|\\hat{\\lambda})\\propto e^{-\\lambda\\theta}\n",
    "$$\n",
    "\n",
    "Likelihood:\n",
    "\n",
    "$$\n",
    "p(D|\\theta) = \\prod_{i=1}^N\\theta e^{-\\theta x_i} = \\theta^Ne^{-\\theta\\sum_{i=1}^N x_i}\n",
    "$$\n",
    "\n",
    "Combining these two terms, we get the posterior:\n",
    "\n",
    "\\begin{aligned}\n",
    "p(\\theta|D, \\hat{\\lambda}) & \\propto \\theta^N e^{-\\theta(\\hat{\\lambda} + \\sum_{i=1}^N x_i)} \\\\\n",
    "& = \\mathrm{Ga}(\\theta|N+1, \\hat{\\lambda}+\\sum_{i=1}^N x_i)\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) Is the exponential prior conjugate to the exponential likelihood?\n",
    "\n",
    "Yes, both terms are conjugate because they have the general form of a Gamma distribution. The prior has the distribution: $p(\\theta|\\hat{\\lambda}) = \\mathrm{Ga}(\\theta|1, \\lambda)$. The likelihood, despite not being a distribution, is proportional to a Gamma distribution: $p(D|\\theta) \\propto \\mathrm{Ga}(\\theta|N+1, \\sum_{i=1}^N x_i$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) Since the posterior is a Gamma function, it's simple to derive the mean:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\theta|D, \\hat{\\lambda}] = \\frac{N+1}{\\hat{\\lambda} + \\sum_{i=1}^N x_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(g) Explain why the MLE and posterior mean differ. Which is more reasonable in this example?\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "To explain the difference between the MLE and the posterior mean, let's rewrite the expression for the posterior mean in a more suitable manner:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\theta|D,\\hat{\\lambda}] = \\frac{N+1}{\\hat{\\lambda}+\\sum_{i=1}^Nx_i} = \\left(\\frac{\\sum_{i=1}^N x_i}{N+1} + \\frac{\\hat{\\lambda}}{N+1}\\right)^{-1}\n",
    "$$\n",
    "\n",
    "The first represents the information obtained thanks to the dataset and the second represends our prior. Moreover, note if $\\hat{\\lambda} = 0$, then we have an uninformative prior and the posterior mean is almost equal to the MLE. If our dataset is huge ($N$ is very large), then the posterior mean converges to the MLE. This happens because the prior term goes to zero and the addition in the denominator becomes irrelevant. \n",
    "\n",
    "In our particular example, since an expert gave us an informative prior ($\\hat{\\lambda} > 3$) and our dataset is small, we should use the posterior mean."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
