{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.20 Class conditional densities for binary data\n",
    "Consider a generative classifier for $C$ classes with class conditional density $p(x|y)$ and uniform class prior $p(y)$. Suppose all the $D$ features are binary, $x_j \\in \\{0, 1\\}$. If we assume all the features are conditionally independent (the naive Bayes assumption), we can write\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}|y=c) = \\prod_{j=1}^D\\mathrm{Ber}(x_j|\\theta_{jc})\n",
    "$$\n",
    "This requires $DC$ parameters.\n",
    "\n",
    "a. Now consider a different model, which we will call the “full” model, in which all the features are fully dependent (i.e., we make no factorization assumptions). How might we represent $p(\\mathbf{x}|y = c)$ in this case? How many parameters are needed to represent $p(\\mathbf{x}|y = c)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**: Use the chain rule of probability:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}|y=c) = p(x_1|y=c)p(x_2|x_1, y=c)\\cdots p(x_D|x_1,\\ldots,x_{D-1}, y=c)\n",
    "$$\n",
    "\n",
    "Here all the factors are conditional probability density functions which is affected by all of its parent nodes in the Bayesian network, so the number of required parameters are $C+2C+\\cdots+2^{D-1}C = C(2^D-1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Assume the number of features $D$ is fixed. Let there be $N$ training cases. If the sample size $N$ is very small, which model (naive Bayes or full) is likely to give lower test set error, and why?\n",
    "\n",
    "**Solution**: For $N$ very small, the naive Bayes model would outperform the full model. This would happen because the full model has too many parameters and would overfit on a small dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) If the sample size N is very large, which model (naive Bayes or full) is likely to give lower test set error, and why?\n",
    "\n",
    "**Solution**: For $N$ very large, the opposite happens. The strong hypothesis of the Naive Bayes would make the model underfit. On the other hand, since there is no hypothesis constraining the full model and we have enough data, this would have a lower test error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) What is the computational complexity of fitting the full and naive Bayes models as a function of $N$ and $D$? Use big-Oh notation. (Fitting the model here means computing the MLE or MAP parameter estimates. You may assume you can convert a $D$-bit vector to an array index in $O(D)$ time.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**: Naive Bayes: $O(ND)$\n",
    "\n",
    "Full Bayes: \n",
    "```\n",
    "procedure TRAINING(dataset)\n",
    "    p_{idx,c} = 0, N_c = 0\n",
    "    for i = 1: N do\n",
    "        c = y_i\n",
    "        N_c = N_c + 1\n",
    "        idx = binaryToIndex(x_i)\n",
    "        p_{idx,c} = p_{idx,c} + 1\n",
    "    end for\n",
    "    p_{idx,c} = p_{idx,c} / N_c\n",
    "    return p_{idx, c}\n",
    "end procedure\n",
    "```\n",
    "The `binaryToIndex` function is the one-to-one correspondence which maps a binary sequence $(x_0,\\ldots,x_i)$ to a single index $\\sum_{k=0}^ix_k2^k$. Since time complexity of this function is at most $O(D)$, the whole time complexity is $O(ND)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) What is the computational complexity of applying the full and naive Bayes models at test time to a single test case?\n",
    "\n",
    "**Solution**: Naive Bayes: $O(CD)$\n",
    "\n",
    "Full Bayes:\n",
    "\n",
    "The whole testing algorithm goes as follows:\n",
    "```\n",
    "procedure TEST(dataset)\n",
    "    idx = binaryToIndex(x)\n",
    "    bestCandidate = 0\n",
    "    class = -1\n",
    "    for c = 1: C do\n",
    "        if bestCandidate < p_{idx, c} then\n",
    "            bestCandidate = p_{idx, c}\n",
    "            class = c\n",
    "        end if\n",
    "    end for\n",
    "    return bestCandidate, class\n",
    "end procedure\n",
    "```\n",
    "\n",
    "The whole algorithm runs within time complexity $O(D) + O(C) = O(\\max(C, D))$. Usually, the number of features is bigger than the number of classes, so runtime complexity is equal to $O(D)$. In other words, what bounds the runtime of the algorithm is the time taken to convert the binary index to the table index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) Suppose the test case has missing data. Let $\\mathbf{x}_v$ be the visible features of size $v$, and $\\mathbf{x}_h$ be the hidden (missing) features of size $h$, where $v + h = D$. What is the computational complexity of computing $p(y|\\mathbf{x}_v,θ)$ for the full and naive Bayes models, as a function of $v$ and $h$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Naive Bayes: $O(CD)$\n",
    "- Full model: $O(D) + O(2^hv) = \\max((v+h), 2^hv)$, which will most likely be equal to $O(2^hv)$. The model suffers a lot in terms of run time, when we have missing features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
