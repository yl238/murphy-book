{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.19 Irrelevant features with naive Bayes\n",
    "(Source: Jaakkola.) Let $x_{iw} = 1$ if word $w$ occurs in document $i$ and $x_{iw} = 0$ otherwise. Let $θ_{cw}$ be the estimated probability that word $w$ occurs in documents of class $c$. Then the log-likelihood that document $x$ belongs to class $c$ is\n",
    "\n",
    "\\begin{aligned}\n",
    "\\log p(\\mathbf{x}_i |c, \\theta) & = \\log\\prod_{w=1}^W\\theta_{cw}^{x_{iw}}(1-\\theta_{cw})^{1-x_{iw}}\\\\\n",
    "& = \\sum_{w=1}^Wx_{iw}\\log\\theta_{cw} + (1-x_{iw})\\log(1-\\theta_{cw})\\\\\n",
    "& = \\sum_{w=1}^Wx_{iw}\\log\\frac{\\theta_{cw}}{1-\\theta_{cw}} + \\sum_w\\log(1-\\theta_{cw})\n",
    "\\end{aligned}\n",
    "\n",
    "where $W$ is the number of words in the vocabulary. We can write this more succintly as\n",
    "\n",
    "$$\n",
    "\\log p(\\mathbf{x}_i|c,\\theta) = \\boldsymbol\\phi(\\mathbf{x}_i)^T\\boldsymbol\\beta_c\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x}_i = (x_{i1},\\ldots, x_{iw})$ is a bit vector, $\\boldsymbol\\phi(\\mathbf{x}_i) = (\\mathbf{x}_i, 1)$, and \n",
    "\n",
    "$$\n",
    "\\boldsymbol\\beta_c = \\left(\\log\\frac{\\theta_{c1}}{1-\\theta_{c1}},\\ldots,\\log\\frac{\\theta_{cW}}{1-\\theta_{cW}}, \\sum_w\\log(1-\\theta_{cw})\\right)^T\n",
    "$$\n",
    "\n",
    "We see that this is a linear classifier, since the class-conditional density is a linear function (an inner product) of the parameters $\\boldsymbol\\beta_c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Assuming $p(C = 1) = p(C = 2) = 0.5$, write down an expression for the log posterior odds ratio, $\\log_2\\frac{p(c=1|x_i)}{p(c=2|x_i)}$, in terms of the features $\\phi(\\mathbf{x}_i)$ and the parameters $β_1$ and $β_2$.\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\log_2\\frac{p(c=1|x_i)}{p(c=2|x_i)} & = \\log_2\\frac{p(x_i|c=1)p(c=1)}{p(x_i|c=2)p(c=2)} \\\\\n",
    "& = \\log_2\\frac{p(x_i|c=1)}{p(x_i|c=2)} \\\\\n",
    "& = \\log_2 p(x_i|c=1) - \\log_2 p(x_i|c=2) \\\\\n",
    "& = \\phi(x_i)^t(\\beta_1-\\beta_2)\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Intuitively, words that occur in both classes are not very “discriminative”, and therefore should not affect our beliefs about the class label. Consider a particular word $w$. State the conditions on $θ_1$, $w$ and $θ_2$, $w$ (or equivalently the conditions on $β_1$, $w$, $β_2$, $w$) under which the presence or absence of $w$ in a test document will have no effect on the class posterior (such a word will be ignored by the classifier). Hint: using your previous result, figure out when the posterior odds ratio is 0.5/0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:\n",
    "\n",
    "The posterior ratios would be 1, therefore\n",
    "\n",
    "\\begin{aligned}\n",
    "\\beta_{1,w} = \\beta_{2,w} \\iff \\log_2\\frac{\\theta_{1,w}}{1-\\theta_{1,w}}= \\log_2\\frac{\\theta_{2,w}}{1-\\theta_{2,w}} & \\iff \\frac{\\theta_{1,w}}{1-\\theta_{1,w}} = \\frac{\\theta_{2,w}}{1-\\theta_{2,w}} \\\\\n",
    "& \\iff \\theta_{1,w} = \\theta_{2,w}\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) The posterior mean estimate of $θ$, using a $\\mathrm{Beta}(1,1)$ prior, is given by\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_{cw} = \\frac{1+\\sum_{i\\in c}x_{iw}}{2+n_c}\n",
    "$$ \n",
    "\n",
    "where the sum is over the $n_c$ documents in class $c$. Consider a particular word $w$, and suppose it always occurs in every document (regardless of class). Let there be $n_1$ documents of class 1 and $n_2$ be the number of documents in class 2, where $n_1 \\ne n_2$ (since e.g., we get much more non-spam than spam; this is an example of class imbalance). If we use the above estimate for $θ_{cw}$, will word $w$ be ignored by our classifier? Explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:\n",
    "\n",
    "The word $w$ occurs in every document and $n_1 \\ne n_2$. Therefore, the posterior mean estimates for the parameters are \n",
    "\n",
    "\\begin{aligned}\n",
    "\\theta_{1,w} & = \\frac{1+\\sum_{i\\in c=1}x_{iw}}{2 + n_1} = \\frac{n_1 + 1}{n_1 + 2}\\\\\n",
    "\\theta_{2,w} & = \\frac{1+\\sum_{i\\in c=2}x_{iw}}{2 + n_2} = \\frac{n_2 + 1}{n_2 + 2}\n",
    "\\end{aligned}\n",
    "\n",
    "The two parameter estimates are equal when \n",
    "\n",
    "$$\n",
    "\\frac{n_1+1}{n_1+2} = \\frac{n_2+1}{n_2+2} \\iff n_1 = n_2\n",
    "$$\n",
    "Since the classes are imbalanced, $\\theta_{1,w}\\ne\\theta_{2,w}$ and the word $w$ will not be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) d. What other ways can you think of which encourage “irrelevant” words to be ignored?\n",
    "\n",
    "**Solution**: Use multinomial NB model which takes into account the number of occurrences of each word in the documents, rather than using Bernoulli NB model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
