{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from IPython import display\n",
    "import os\n",
    "from scipy.misc import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.fc1(x)\n",
    "        output = F.relu(output)\n",
    "        output = self.out(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(MNIST('mnist-data/', train=True, download=True,\n",
    "                               transform=transforms.Compose([transforms.ToTensor(),])),\n",
    "                          batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(MNIST('mnist-data/', train=False, \n",
    "                               transform=transforms.Compose([transforms.ToTensor(),])),\n",
    "                        batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLP(28*28, 1024, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "from pyro.distributions import Normal, Categorical\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_softmax = nn.LogSoftmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x_data, y_data):\n",
    "    fc1w_prior = Normal(loc=torch.zeros_like(net.fc1.weight), scale=torch.ones_like(net.fc1.weight))\n",
    "    fc1b_prior = Normal(loc=torch.zeros_like(net.fc1.bias), scale=torch.ones_like(net.fc1.bias))\n",
    "    \n",
    "    outw_prior = Normal(loc=torch.zeros_like(net.out.weight), scale=torch.ones_like(net.out.weight))\n",
    "    outb_prior = Normal(loc=torch.zeros_like(net.out.bias), scale=torch.ones_like(net.out.bias))\n",
    "    \n",
    "    priors = {'fc1.weight': fc1w_prior, 'fc1.bias': fc1b_prior, 'out.weight': outw_prior,\n",
    "             'out.bias': outb_prior}\n",
    "    # Lift module parameters to random variables sampled from the priors\n",
    "    lifted_module = pyro.random_module('module', net, priors)\n",
    "    # sample a regressor (which also samples w and b)\n",
    "    lifted_reg_model = lifted_module()\n",
    "    \n",
    "    lhat = log_softmax(lifted_reg_model(x_data))\n",
    "    \n",
    "    pyro.sample('obs', Categorical(logits=lhat), obs=y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "softplus = torch.nn.Softplus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guide(x_data, y_data):\n",
    "    # First layer weight distribution priors\n",
    "    fc1w_mu = torch.randn_like(net.fc1.weight)\n",
    "    fc1w_sigma = torch.randn_like(net.fc1.weight)\n",
    "    \n",
    "    fc1w_mu_param = pyro.param('fc1w_mu', fc1w_mu)\n",
    "    fc1w_sigma_param = softplus(pyro.param('fc1w_sigma', fc1w_sigma))\n",
    "    fc1w_prior = Normal(loc=fc1w_mu_param, scale=fc1w_sigma_param)\n",
    "    \n",
    "    # First layer bias distribution priors\n",
    "    fc1b_mu = torch.randn_like(net.fc1.bias)\n",
    "    fc1b_sigma = torch.randn_like(net.fc1.bias)\n",
    "    \n",
    "    fc1b_mu_param = pyro.param(\"fc1b_mu\", fc1b_mu)\n",
    "    fc1b_sigma_param = softplus(pyro.param(\"fc1b_sigma\", fc1b_sigma))\n",
    "    fc1b_prior = Normal(loc=fc1b_mu_param, scale=fc1b_sigma_param)\n",
    "    \n",
    "    # Output layer weight distribution priors\n",
    "    outw_mu = torch.randn_like(net.out.weight)\n",
    "    outw_sigma = torch.randn_like(net.out.weight)\n",
    "    \n",
    "    outw_mu_param = pyro.param(\"outw_mu\", outw_mu)\n",
    "    outw_sigma_param = softplus(pyro.param(\"outw_sigma\", outw_sigma))\n",
    "    outw_prior = Normal(loc=outw_mu_param, scale=outw_sigma_param).independent(1)\n",
    "    \n",
    "    # Output layer bias distribution priors\n",
    "    outb_mu = torch.randn_like(net.out.bias)\n",
    "    outb_sigma = torch.randn_like(net.out.bias)\n",
    "    \n",
    "    outb_mu_param = pyro.param(\"outb_mu\", outb_mu)\n",
    "    outb_sigma_param = softplus(pyro.param(\"outb_sigma\", outb_sigma))\n",
    "    outb_prior = Normal(loc=outb_mu_param, scale=outb_sigma_param)\n",
    "    priors = {'fc1.weight': fc1w_prior, 'fc1.bias': fc1b_prior, \n",
    "              'out.weight': outw_prior, 'out.bias': outb_prior}\n",
    "    \n",
    "    lifted_module = pyro.random_module('module', net, priors)\n",
    "    \n",
    "    return lifted_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam({'lr': 0.01})\n",
    "svi = SVI(model, guide, optim, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 loss  2097.9012491882086\n",
      "Epoch  1 loss  371.22492424211504\n",
      "Epoch  2 loss  157.5576159573873\n",
      "Epoch  3 loss  109.70278435794512\n",
      "Epoch  4 loss  95.79310842116674\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 5\n",
    "loss = 0\n",
    "\n",
    "for j in range(num_iterations):\n",
    "    loss = 0\n",
    "    for batch_id, data in enumerate(train_loader):\n",
    "        # Calculate the loss and take a gradient step\n",
    "        loss += svi.step(data[0].view(-1, 28*28), data[1])\n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = loss / normalizer_train\n",
    "    \n",
    "    print(\"Epoch \", j, \"loss \", total_epoch_loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction when network is forced to predict\n",
      "accuracy: 89 %\n"
     ]
    }
   ],
   "source": [
    "num_samples = 10\n",
    "def predict(X):\n",
    "    sampled_models = [guide(None, None) for _ in range(num_samples)]\n",
    "    yhats = [model(X).data for model in sampled_models]\n",
    "    mean = torch.mean(torch.stack(yhats), 0)\n",
    "    return np.argmax(mean.numpy(), axis=1)\n",
    "\n",
    "print('Prediction when network is forced to predict')\n",
    "correct = 0\n",
    "total = 0\n",
    "for j, data in enumerate(test_loader):\n",
    "    images, labels = data\n",
    "    predicted = predict(images.view(-1, 28*28))\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels.numpy()).sum().item()\n",
    "print('accuracy: %d %%' %(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5\n",
    "    np_img = img.numpy()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(1, 1))\n",
    "    ax.imshow(np_img, cmap='gray', interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100\n",
    "def give_uncertainties(x):\n",
    "    sampled_models = [guide(None, None) for _ in range(num_samples)]\n",
    "    yhats = [F.log_softmax(model(x.view(-1, 28*28)).data, 1).detach().numpy() for model in sampled_models]\n",
    "    return np.asarray(yhats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch(images, labels, plot=True, prob_thresh=0.2):\n",
    "    y = give_uncertainties(images)\n",
    "    predicted_for_images = 0\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        if plot:\n",
    "            print('Real: ', labels[i].item())\n",
    "            fig, axes = plt.subplots(1, 10, sharey=True, figsize=(20, 2))\n",
    "            \n",
    "        all_digits_prob = []\n",
    "        \n",
    "        highlighted_something = False\n",
    "        \n",
    "        for j in range(len(classes)):\n",
    "            highlight = False\n",
    "            histo = []\n",
    "            histo_exp = []\n",
    "            for z in range(y.shape[0]):\n",
    "                histo.append(y[z][i][j])\n",
    "                histo_exp.append(np.exp(y[z][i][j]))\n",
    "            \n",
    "            prob = np.percentile(histo_exp, 50) # sampling median probability\n",
    "            \n",
    "            if prob > prob_thresh: # Select if network thinks this sample has 20% of being a label\n",
    "                highlight = True # possibly an answer\n",
    "            \n",
    "            all_digits_prob.append(prob)\n",
    "            \n",
    "            if plot:\n",
    "                N, bins, patches = axes[j].hist(histo, bins=8, color='lightgray', lw=0, weights=np.ones(len(histo))/len(histo),\n",
    "                                               density=False)\n",
    "                axes[j].set_title(str(j) + \" (\" + str(round(prob, 2)) + \")\")\n",
    "            if highlight:\n",
    "                highlighted_something = True\n",
    "                if plot:\n",
    "                    # We'll color code by height,\n",
    "                    fracs = N / N.max()\n",
    "                    \n",
    "                    # We need to normalize the data to 0..1 for the full range of the colormap\n",
    "                    norm = colors.Normalize(fracs.min(), fracs.max())\n",
    "                    \n",
    "                    # Now, we'll loop through our objects and set the color of each accordingly\n",
    "                    for frac, patch in zip(fracs, patches):\n",
    "                        color = plt.cm.viridis(norm(frac))\n",
    "                        patch.set_facecolor(color)\n",
    "        if plot:\n",
    "            plt.show()\n",
    "            \n",
    "        predicted = np.argmax(all_digits_prob)\n",
    "        \n",
    "        if highlighted_something:\n",
    "            predicted_for_images += 1\n",
    "            if labels[i].item == predicted:\n",
    "                if plot:\n",
    "                    print('Correct')\n",
    "                correct_predictions += 1.0\n",
    "            else:\n",
    "                if plot:\n",
    "                    print('Incorrect :()')\n",
    "        else:\n",
    "            if plot:\n",
    "                print('Undecided.')\n",
    "        if plot:\n",
    "            imshow(images[i].squeeze())\n",
    "        \n",
    "    if plot:\n",
    "        print('Summary')\n",
    "        print('Total images: {}'.format(len(labels)))\n",
    "        print('Predicted for: ', predicted_for_images)\n",
    "        print('Accuracy when predicted: ', correct_predictions / predicted_for_images)\n",
    "    \n",
    "    return len(labels), correct_predictions, predicted_for_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction when network can refuse\n",
      "Total images:  10000\n",
      "Skipped:  1211\n",
      "Accuracy when made predictions: 0 %\n"
     ]
    }
   ],
   "source": [
    "# Prediction when network can decide not to predict\n",
    "print('Prediction when network can refuse')\n",
    "correct = 0\n",
    "total = 0\n",
    "total_predicted_for = 0\n",
    "for j, data in enumerate(test_loader):\n",
    "    images, labels = data\n",
    "    \n",
    "    total_minibatch, correct_minibatch, predictions_minibatch = test_batch(images, labels, plot=False)\n",
    "    total += total_minibatch\n",
    "    correct += correct_minibatch\n",
    "    total_predicted_for += predictions_minibatch\n",
    "\n",
    "print('Total images: ', total)\n",
    "print('Skipped: ', total-total_predicted_for)\n",
    "print('Accuracy when made predictions: %d %%' % (100 * correct / total_predicted_for))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
